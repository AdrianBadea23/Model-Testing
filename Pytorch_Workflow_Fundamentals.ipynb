{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VMIDhPSdvmVG"
      },
      "outputs": [],
      "source": [
        "# PyTorch Workflow - end to end workflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "covering = {1: \"data (prepare and load)\",\n",
        "            2: \"build model\",\n",
        "            3: \"fitting model to data(training)\",\n",
        "            4: \"making predictions\",\n",
        "            5: \"save + load\",\n",
        "            6: \"put all together\"}"
      ],
      "metadata": {
        "id": "PfvzmOflwGSS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn # pytorch building blocks for nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jfPzTEmAwnIl",
        "outputId": "e4153c33-5b0a-4934-9f0f-ac9d09bc4b0b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.3.1+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparing and Loading\n",
        "# Data = almost anything(excel spreadsheet, Images, Videos, Audio, DNA, Text)\n",
        "# 1. Get data into numerical representation\n",
        "# 2. Build a model to learn patterns in that numerical representaion\n",
        "\n",
        "# Linear Regression to make a straight line with known parameters"
      ],
      "metadata": {
        "id": "XGQnJ2AOxK1k"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight = 0.7\n",
        "bias = 0.3\n",
        "\n",
        "# Create data\n",
        "\n",
        "start = 0\n",
        "end = 1\n",
        "step = 0.02\n",
        "X = torch.arange(start, end, step).unsqueeze(dim=1)\n",
        "y = weight * X + bias\n",
        "\n",
        "X[:10], y[:10], len(X), len(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmHDej1dykTO",
        "outputId": "cc1f6958-c248-4961-c00a-ff70112fe3fd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.0000],\n",
              "         [0.0200],\n",
              "         [0.0400],\n",
              "         [0.0600],\n",
              "         [0.0800],\n",
              "         [0.1000],\n",
              "         [0.1200],\n",
              "         [0.1400],\n",
              "         [0.1600],\n",
              "         [0.1800]]),\n",
              " tensor([[0.3000],\n",
              "         [0.3140],\n",
              "         [0.3280],\n",
              "         [0.3420],\n",
              "         [0.3560],\n",
              "         [0.3700],\n",
              "         [0.3840],\n",
              "         [0.3980],\n",
              "         [0.4120],\n",
              "         [0.4260]]),\n",
              " 50,\n",
              " 50)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting data into training and test sets"
      ],
      "metadata": {
        "id": "XgZIseaxzA2O"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create test/test split\n",
        "\n",
        "train_split = int(0.8 * len(X))\n",
        "X_train, y_train = X[:train_split], y[:train_split]\n",
        "X_test, y_test = X[train_split:], y[train_split:]\n",
        "\n",
        "len(X_train), len(y_train), len(X_test), len(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWpSnyWNzt-A",
        "outputId": "12629357-5dd0-4fe3-9056-5ae8693f7776"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40, 40, 10, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(train_data = X_train, train_labels = y_train, test_data = X_test, test_labels = y_test, predictions = None):\n",
        "  plt.figure(figsize=(10, 7))\n",
        "  plt.scatter(train_data, train_labels, c=\"b\", s = 4, label=\"Training data\")\n",
        "  plt.scatter(test_data, test_labels, c=\"g\", s = 4, label=\"Testing data\")\n",
        "\n",
        "  if predictions is not None:\n",
        "    plt.scatter(test_data, predictions, c = \"r\", s = 4, label=\"Predictions\")\n",
        "\n",
        "plt.legend(prop={\"size\":14})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "Gb6pMelI1E0g",
        "outputId": "f1f263e2-46c3-484e-e55d-4d988ca058ac"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x784e5c191e70>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "Nm3Opsju1eCd",
        "outputId": "e8f8d96c-0fdb-432f-ef74-624cec85e7f5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a Pytorch model\n",
        "# What our model does: start with random values weight and bias, look at training data and adjust the random values to get closer to the ideal values(the weight and bias values we used to create the data)\n",
        "# How does it do so: 1. Gradient descent and 2. Backpropagation"
      ],
      "metadata": {
        "id": "wEQofvcd25gt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Linear Regression model class\n",
        "\n",
        "class LinearRegressionModel(nn.Module): #<- almost everything in PyTorch inherits from nn.Module\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.weights = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
        "    self.bias = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
        "\n",
        "  # Forward method to define the computaion in the model\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor: #<- \"x\" is the input data\n",
        "    return self.weights * x + self.bias"
      ],
      "metadata": {
        "id": "PYQ0E_iK4B3T"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch Model building essentials\n",
        "# torch.nn - contains all the building blocks for computational graphs\n",
        "# torch.nn.Parameter - what params should out model try and learn\n",
        "# torch.nn.Module - the base class for all nn modules, if you subclass it you should overwrite forward()\n",
        "# torch.optim - optimizers in pytorch, help with gradient descent\n",
        "# def forward() - all nn.Module subclasses require you to overwrite forward(), this method defines what happens in the forward computation"
      ],
      "metadata": {
        "id": "dlg7D52uHMNB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the contents of our pytorch model\n",
        "# Check our model params or check what is inside our model with .parameters()"
      ],
      "metadata": {
        "id": "KNQE35iCIidm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create random seed\n",
        "\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "# Create an instance of our model\n",
        "\n",
        "model_0 = LinearRegressionModel()\n",
        "list(model_0.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90EhEJSNi-IR",
        "outputId": "d73d012a-e2d7-4ded-d77a-b02f2c74e56e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([0.0461], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([0.4024], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List named parameters\n",
        "\n",
        "model_0.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDY3Ut5QjO9N",
        "outputId": "a3813605-2f0f-4d93-f5f2-9623b5a79600"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.0461])), ('bias', tensor([0.4024]))])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions using torch.inference_mode()\n",
        "# To check our models predictive power lets check how it predicts x_test and y_test\n",
        "# When we pass data through out model its goint through the forward method"
      ],
      "metadata": {
        "id": "iDgy2WutkAjW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode(): # context manager - inference mode turns off gradient tracking - pytorch is keeping track of less data => faster predictions\n",
        "  y_preds = model_0(X_test)\n",
        "\n",
        "y_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpKivAcDlAOH",
        "outputId": "ed40a8ed-f6f1-4c34-cf9a-d6df9f2d2cd1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4393],\n",
              "        [0.4402],\n",
              "        [0.4412],\n",
              "        [0.4421],\n",
              "        [0.4430],\n",
              "        [0.4439],\n",
              "        [0.4448],\n",
              "        [0.4458],\n",
              "        [0.4467],\n",
              "        [0.4476]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(predictions = y_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "_Gc22T-mlLgb",
        "outputId": "4300bebf-bba0-4206-b5aa-f58526650455"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model - the whole idea of training is to get the model to move from some random params to some known parameters\n",
        "# Poor representaion -> Better representation\n",
        "# One way to measure how poor or how wrong your models predictions are is to use a loss function\n",
        "# Loss function may also be called cost funtion or criterion in deifferent areas\n",
        "# Things we need to train:\n",
        "# Loss function: A function to mesure how wrong your models predictions are compared with the ideal outputs\n",
        "# Optimizer: Takes into account the loss of a model and adjusts the models parameters eg werights and bias to improve the loss function\n",
        "# For pytorch we need a train loop and a testing loop"
      ],
      "metadata": {
        "id": "g-qxu54hl2X6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup loss function\n",
        "\n",
        "loss_fn = nn.L1Loss()\n",
        "\n",
        "# Setup optimizer\n",
        "\n",
        "optimizer = torch.optim.SGD(params = model_0.parameters(), lr = 0.001) #lr = learning rate"
      ],
      "metadata": {
        "id": "S9iz6csgpM7S"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a training plus a testing loop in PyTorch\n",
        "# A couple of things we need in a training loop\n",
        "# 0. Loop through the data\n",
        "# 1. Forward pass (this involves data moving through our models forward function) to make predictions - also called forward propagation\n",
        "# 2. Calculate the loss (compare forward pass predictions to ground truth)\n",
        "# 3. Optimizer zero grad\n",
        "# 4. Loss backward - move backwards through the network to calculate the gradients of each of the parameters of our model with respect to the loss\n",
        "# 5. Optimizer step - use the optimizer to adjust our model's parameters to try and improve the loss (gradient descent)"
      ],
      "metadata": {
        "id": "5EbRTW30qk6p"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# An epoch is one loop through the data (hyperparameter because we've set them)\n",
        "epochs = 5000\n",
        "\n",
        "epoch_count = []\n",
        "loss_values = []\n",
        "test_loss_values = []\n",
        "\n",
        "# Training\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  # Set the model to training mode\n",
        "  model_0.train() # train mode in PyTorch sets all parameters that require gradients to require gradients\n",
        "  # 1. Forward pass\n",
        "  y_pred = model_0(X_train)\n",
        "\n",
        "  # 2. Calculate loss\n",
        "  loss = loss_fn(y_pred, y_train)\n",
        "\n",
        "  # 3. Optimizer zero grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # 4. Perform backpropagation on the loss with respect to the parameters of the model\n",
        "  loss.backward()\n",
        "\n",
        "  # 5. Step the optimizer\n",
        "  optimizer.step() # by default how the optimizer changes will accumulate through the loop we have to zero them above so it doesnt accumulate and start fresh each loop\n",
        "  model_0.eval() # turns off different settings not needed for evaluation/testing\n",
        "  with torch.inference_mode(): # turns off gradient tracking\n",
        "    # 1. forward pass\n",
        "    test_pred = model_0(X_test)\n",
        "    # 2. Calculate loss\n",
        "    test_loss = loss_fn(test_pred, y_test)\n",
        "  # Print out what's happening\n",
        "    epoch_count.append(epoch)\n",
        "    loss_values.append(loss)\n",
        "    test_loss_values.append(test_loss)\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"Epoch: {epoch} | Test: {loss} | Test loss: {test_loss}\")\n"
      ],
      "metadata": {
        "id": "EAP40VJEuerS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "8309af28-f5c8-4d15-f86e-9b707dff5d3f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Test: 0.175259068608284 | Test loss: 0.47861891984939575\n",
            "Epoch: 10 | Test: 0.1707487404346466 | Test loss: 0.4697968363761902\n",
            "Epoch: 20 | Test: 0.16647057235240936 | Test loss: 0.4611462652683258\n",
            "Epoch: 30 | Test: 0.16249912977218628 | Test loss: 0.4528437554836273\n",
            "Epoch: 40 | Test: 0.15880891680717468 | Test loss: 0.44477328658103943\n",
            "Epoch: 50 | Test: 0.1552494317293167 | Test loss: 0.43687963485717773\n",
            "Epoch: 60 | Test: 0.1521022468805313 | Test loss: 0.4293982982635498\n",
            "Epoch: 70 | Test: 0.14895504713058472 | Test loss: 0.4219169020652771\n",
            "Epoch: 80 | Test: 0.14621928334236145 | Test loss: 0.4149734377861023\n",
            "Epoch: 90 | Test: 0.1435694694519043 | Test loss: 0.4080897271633148\n",
            "Epoch: 100 | Test: 0.14101549983024597 | Test loss: 0.40138810873031616\n",
            "Epoch: 110 | Test: 0.13881707191467285 | Test loss: 0.3951113224029541\n",
            "Epoch: 120 | Test: 0.13661864399909973 | Test loss: 0.38883456587791443\n",
            "Epoch: 130 | Test: 0.13455162942409515 | Test loss: 0.3828040659427643\n",
            "Epoch: 140 | Test: 0.13275764882564545 | Test loss: 0.3771430552005768\n",
            "Epoch: 150 | Test: 0.13096368312835693 | Test loss: 0.37148210406303406\n",
            "Epoch: 160 | Test: 0.12921786308288574 | Test loss: 0.365945965051651\n",
            "Epoch: 170 | Test: 0.12778058648109436 | Test loss: 0.3609093427658081\n",
            "Epoch: 180 | Test: 0.12634329497814178 | Test loss: 0.3558727204799652\n",
            "Epoch: 190 | Test: 0.1249060407280922 | Test loss: 0.3508361279964447\n",
            "Epoch: 200 | Test: 0.12363733351230621 | Test loss: 0.34617963433265686\n",
            "Epoch: 210 | Test: 0.12250832468271255 | Test loss: 0.34177660942077637\n",
            "Epoch: 220 | Test: 0.1213793158531189 | Test loss: 0.3373735547065735\n",
            "Epoch: 230 | Test: 0.12025029957294464 | Test loss: 0.3329704999923706\n",
            "Epoch: 240 | Test: 0.11923469603061676 | Test loss: 0.3288887143135071\n",
            "Epoch: 250 | Test: 0.11836455762386322 | Test loss: 0.3251281678676605\n",
            "Epoch: 260 | Test: 0.11749438941478729 | Test loss: 0.32136765122413635\n",
            "Epoch: 270 | Test: 0.11662423610687256 | Test loss: 0.3176070749759674\n",
            "Epoch: 280 | Test: 0.11575408279895782 | Test loss: 0.3138464689254761\n",
            "Epoch: 290 | Test: 0.11499643325805664 | Test loss: 0.31047672033309937\n",
            "Epoch: 300 | Test: 0.11433474719524384 | Test loss: 0.3073675036430359\n",
            "Epoch: 310 | Test: 0.11367306858301163 | Test loss: 0.3042582869529724\n",
            "Epoch: 320 | Test: 0.11301138252019882 | Test loss: 0.3011491000652313\n",
            "Epoch: 330 | Test: 0.11234970390796661 | Test loss: 0.29803991317749023\n",
            "Epoch: 340 | Test: 0.11168801784515381 | Test loss: 0.29493072628974915\n",
            "Epoch: 350 | Test: 0.11109445244073868 | Test loss: 0.292151540517807\n",
            "Epoch: 360 | Test: 0.11058981716632843 | Test loss: 0.2897023558616638\n",
            "Epoch: 370 | Test: 0.11008520424365997 | Test loss: 0.28725314140319824\n",
            "Epoch: 380 | Test: 0.10958057641983032 | Test loss: 0.2848038971424103\n",
            "Epoch: 390 | Test: 0.10907596349716187 | Test loss: 0.2823547422885895\n",
            "Epoch: 400 | Test: 0.10857133567333221 | Test loss: 0.2799055278301239\n",
            "Epoch: 410 | Test: 0.10806673765182495 | Test loss: 0.2774563431739807\n",
            "Epoch: 420 | Test: 0.1075621098279953 | Test loss: 0.27500709891319275\n",
            "Epoch: 430 | Test: 0.10707239806652069 | Test loss: 0.2726917862892151\n",
            "Epoch: 440 | Test: 0.10667239129543304 | Test loss: 0.2709117531776428\n",
            "Epoch: 450 | Test: 0.1062723770737648 | Test loss: 0.26913172006607056\n",
            "Epoch: 460 | Test: 0.10587237030267715 | Test loss: 0.2673516869544983\n",
            "Epoch: 470 | Test: 0.1054723709821701 | Test loss: 0.265571653842926\n",
            "Epoch: 480 | Test: 0.10507235676050186 | Test loss: 0.26379162073135376\n",
            "Epoch: 490 | Test: 0.10467235743999481 | Test loss: 0.2620115578174591\n",
            "Epoch: 500 | Test: 0.10427234321832657 | Test loss: 0.26023155450820923\n",
            "Epoch: 510 | Test: 0.10387233644723892 | Test loss: 0.25845152139663696\n",
            "Epoch: 520 | Test: 0.10347232967615128 | Test loss: 0.2566714882850647\n",
            "Epoch: 530 | Test: 0.10307232290506363 | Test loss: 0.2548914849758148\n",
            "Epoch: 540 | Test: 0.10267231613397598 | Test loss: 0.25311142206192017\n",
            "Epoch: 550 | Test: 0.10227231681346893 | Test loss: 0.2513313889503479\n",
            "Epoch: 560 | Test: 0.10187230259180069 | Test loss: 0.24955137073993683\n",
            "Epoch: 570 | Test: 0.10151398181915283 | Test loss: 0.248381569981575\n",
            "Epoch: 580 | Test: 0.10116495937108994 | Test loss: 0.24727964401245117\n",
            "Epoch: 590 | Test: 0.10081595182418823 | Test loss: 0.24617762863636017\n",
            "Epoch: 600 | Test: 0.10046692937612534 | Test loss: 0.24507570266723633\n",
            "Epoch: 610 | Test: 0.10011792182922363 | Test loss: 0.24397368729114532\n",
            "Epoch: 620 | Test: 0.09976889938116074 | Test loss: 0.24287176132202148\n",
            "Epoch: 630 | Test: 0.09941988438367844 | Test loss: 0.24176974594593048\n",
            "Epoch: 640 | Test: 0.09907086938619614 | Test loss: 0.24066781997680664\n",
            "Epoch: 650 | Test: 0.09872185438871384 | Test loss: 0.23956580460071564\n",
            "Epoch: 660 | Test: 0.09837283939123154 | Test loss: 0.2384638786315918\n",
            "Epoch: 670 | Test: 0.09802382439374924 | Test loss: 0.2373618632555008\n",
            "Epoch: 680 | Test: 0.09767480939626694 | Test loss: 0.23625993728637695\n",
            "Epoch: 690 | Test: 0.09732579439878464 | Test loss: 0.23515792191028595\n",
            "Epoch: 700 | Test: 0.09697677940130234 | Test loss: 0.2340559959411621\n",
            "Epoch: 710 | Test: 0.09662776440382004 | Test loss: 0.2329539805650711\n",
            "Epoch: 720 | Test: 0.09627874195575714 | Test loss: 0.23185205459594727\n",
            "Epoch: 730 | Test: 0.09592973440885544 | Test loss: 0.23075003921985626\n",
            "Epoch: 740 | Test: 0.09558071196079254 | Test loss: 0.22964811325073242\n",
            "Epoch: 750 | Test: 0.09523170441389084 | Test loss: 0.22854609787464142\n",
            "Epoch: 760 | Test: 0.09488268196582794 | Test loss: 0.22744417190551758\n",
            "Epoch: 770 | Test: 0.09453367441892624 | Test loss: 0.22634215652942657\n",
            "Epoch: 780 | Test: 0.09418465197086334 | Test loss: 0.22524023056030273\n",
            "Epoch: 790 | Test: 0.09383564442396164 | Test loss: 0.22413821518421173\n",
            "Epoch: 800 | Test: 0.09348662197589874 | Test loss: 0.2230362892150879\n",
            "Epoch: 810 | Test: 0.09313761442899704 | Test loss: 0.2219342738389969\n",
            "Epoch: 820 | Test: 0.09278859198093414 | Test loss: 0.22083234786987305\n",
            "Epoch: 830 | Test: 0.09243958443403244 | Test loss: 0.21973033249378204\n",
            "Epoch: 840 | Test: 0.09209056198596954 | Test loss: 0.2186284065246582\n",
            "Epoch: 850 | Test: 0.09174155443906784 | Test loss: 0.2175263911485672\n",
            "Epoch: 860 | Test: 0.09139253199100494 | Test loss: 0.21642446517944336\n",
            "Epoch: 870 | Test: 0.09104352444410324 | Test loss: 0.21532244980335236\n",
            "Epoch: 880 | Test: 0.09069450199604034 | Test loss: 0.21422052383422852\n",
            "Epoch: 890 | Test: 0.09034549444913864 | Test loss: 0.2131185084581375\n",
            "Epoch: 900 | Test: 0.08999647200107574 | Test loss: 0.21201658248901367\n",
            "Epoch: 910 | Test: 0.08964746445417404 | Test loss: 0.21091456711292267\n",
            "Epoch: 920 | Test: 0.08929844200611115 | Test loss: 0.20981264114379883\n",
            "Epoch: 930 | Test: 0.08894943445920944 | Test loss: 0.20871062576770782\n",
            "Epoch: 940 | Test: 0.08860041201114655 | Test loss: 0.20760869979858398\n",
            "Epoch: 950 | Test: 0.08825140446424484 | Test loss: 0.20650668442249298\n",
            "Epoch: 960 | Test: 0.08790363371372223 | Test loss: 0.2054734230041504\n",
            "Epoch: 970 | Test: 0.0875597894191742 | Test loss: 0.20464615523815155\n",
            "Epoch: 980 | Test: 0.08721629530191422 | Test loss: 0.20388758182525635\n",
            "Epoch: 990 | Test: 0.08687330782413483 | Test loss: 0.2030603140592575\n",
            "Epoch: 1000 | Test: 0.0865294337272644 | Test loss: 0.20223304629325867\n",
            "Epoch: 1010 | Test: 0.08618611097335815 | Test loss: 0.20147445797920227\n",
            "Epoch: 1020 | Test: 0.08584294468164444 | Test loss: 0.20064720511436462\n",
            "Epoch: 1030 | Test: 0.08549906313419342 | Test loss: 0.19981995224952698\n",
            "Epoch: 1040 | Test: 0.0851559042930603 | Test loss: 0.19906136393547058\n",
            "Epoch: 1050 | Test: 0.08481257408857346 | Test loss: 0.19823411107063293\n",
            "Epoch: 1060 | Test: 0.08446868509054184 | Test loss: 0.1974068582057953\n",
            "Epoch: 1070 | Test: 0.08412571251392365 | Test loss: 0.1966482698917389\n",
            "Epoch: 1080 | Test: 0.08378221094608307 | Test loss: 0.19582100212574005\n",
            "Epoch: 1090 | Test: 0.08343835920095444 | Test loss: 0.19506244361400604\n",
            "Epoch: 1100 | Test: 0.08309551328420639 | Test loss: 0.1942351758480072\n",
            "Epoch: 1110 | Test: 0.08275184780359268 | Test loss: 0.19340792298316956\n",
            "Epoch: 1120 | Test: 0.08240815252065659 | Test loss: 0.19264933466911316\n",
            "Epoch: 1130 | Test: 0.08206532150506973 | Test loss: 0.1918220818042755\n",
            "Epoch: 1140 | Test: 0.0817214772105217 | Test loss: 0.19099481403827667\n",
            "Epoch: 1150 | Test: 0.08137796819210052 | Test loss: 0.19023624062538147\n",
            "Epoch: 1160 | Test: 0.08103500306606293 | Test loss: 0.18940897285938263\n",
            "Epoch: 1170 | Test: 0.08069111406803131 | Test loss: 0.18858173489570618\n",
            "Epoch: 1180 | Test: 0.08034776151180267 | Test loss: 0.18782314658164978\n",
            "Epoch: 1190 | Test: 0.08000463992357254 | Test loss: 0.18699589371681213\n",
            "Epoch: 1200 | Test: 0.07966074347496033 | Test loss: 0.1861686259508133\n",
            "Epoch: 1210 | Test: 0.07931756973266602 | Test loss: 0.1854100227355957\n",
            "Epoch: 1220 | Test: 0.07897426187992096 | Test loss: 0.18458278477191925\n",
            "Epoch: 1230 | Test: 0.07863038033246994 | Test loss: 0.1837555468082428\n",
            "Epoch: 1240 | Test: 0.07828738540410995 | Test loss: 0.1829969584941864\n",
            "Epoch: 1250 | Test: 0.07794389873743057 | Test loss: 0.18216967582702637\n",
            "Epoch: 1260 | Test: 0.07760001718997955 | Test loss: 0.18141110241413116\n",
            "Epoch: 1270 | Test: 0.0772571712732315 | Test loss: 0.18058384954929352\n",
            "Epoch: 1280 | Test: 0.07691353559494019 | Test loss: 0.17975656688213348\n",
            "Epoch: 1290 | Test: 0.0765698254108429 | Test loss: 0.17899799346923828\n",
            "Epoch: 1300 | Test: 0.07622697204351425 | Test loss: 0.17817075550556183\n",
            "Epoch: 1310 | Test: 0.0758831650018692 | Test loss: 0.177343487739563\n",
            "Epoch: 1320 | Test: 0.07553962618112564 | Test loss: 0.1765848845243454\n",
            "Epoch: 1330 | Test: 0.07519669085741043 | Test loss: 0.17575763165950775\n",
            "Epoch: 1340 | Test: 0.07485280185937881 | Test loss: 0.1749303787946701\n",
            "Epoch: 1350 | Test: 0.07450942695140839 | Test loss: 0.1741718351840973\n",
            "Epoch: 1360 | Test: 0.07416631281375885 | Test loss: 0.17334455251693726\n",
            "Epoch: 1370 | Test: 0.07382243871688843 | Test loss: 0.17251726984977722\n",
            "Epoch: 1380 | Test: 0.07347923517227173 | Test loss: 0.17175869643688202\n",
            "Epoch: 1390 | Test: 0.07313594967126846 | Test loss: 0.17093145847320557\n",
            "Epoch: 1400 | Test: 0.07279206812381744 | Test loss: 0.17010419070720673\n",
            "Epoch: 1410 | Test: 0.07244904339313507 | Test loss: 0.16934561729431152\n",
            "Epoch: 1420 | Test: 0.07210559397935867 | Test loss: 0.16851834952831268\n",
            "Epoch: 1430 | Test: 0.07176170498132706 | Test loss: 0.16769108176231384\n",
            "Epoch: 1440 | Test: 0.07141883671283722 | Test loss: 0.16693253815174103\n",
            "Epoch: 1450 | Test: 0.07107521593570709 | Test loss: 0.1661052405834198\n",
            "Epoch: 1460 | Test: 0.07073149085044861 | Test loss: 0.1653466671705246\n",
            "Epoch: 1470 | Test: 0.07038865983486176 | Test loss: 0.16451938450336456\n",
            "Epoch: 1480 | Test: 0.0700448527932167 | Test loss: 0.1636921614408493\n",
            "Epoch: 1490 | Test: 0.06970129162073135 | Test loss: 0.1629335880279541\n",
            "Epoch: 1500 | Test: 0.06935836374759674 | Test loss: 0.16210630536079407\n",
            "Epoch: 1510 | Test: 0.06901448220014572 | Test loss: 0.16127903759479523\n",
            "Epoch: 1520 | Test: 0.0686710998415947 | Test loss: 0.16052047908306122\n",
            "Epoch: 1530 | Test: 0.06832800805568695 | Test loss: 0.15969321131706238\n",
            "Epoch: 1540 | Test: 0.06798411905765533 | Test loss: 0.15886595845222473\n",
            "Epoch: 1550 | Test: 0.06764090061187744 | Test loss: 0.15810738503932953\n",
            "Epoch: 1560 | Test: 0.06729765236377716 | Test loss: 0.1572801172733307\n",
            "Epoch: 1570 | Test: 0.06695375591516495 | Test loss: 0.15645286440849304\n",
            "Epoch: 1580 | Test: 0.06661070138216019 | Test loss: 0.15569427609443665\n",
            "Epoch: 1590 | Test: 0.06626726686954498 | Test loss: 0.1548670083284378\n",
            "Epoch: 1600 | Test: 0.06592338532209396 | Test loss: 0.15403977036476135\n",
            "Epoch: 1610 | Test: 0.06558050960302353 | Test loss: 0.15328118205070496\n",
            "Epoch: 1620 | Test: 0.0652369037270546 | Test loss: 0.1524539291858673\n",
            "Epoch: 1630 | Test: 0.06489366292953491 | Test loss: 0.15166102349758148\n",
            "Epoch: 1640 | Test: 0.06454978883266449 | Test loss: 0.15083377063274384\n",
            "Epoch: 1650 | Test: 0.06420664489269257 | Test loss: 0.15007519721984863\n",
            "Epoch: 1660 | Test: 0.06386329978704453 | Test loss: 0.149247944355011\n",
            "Epoch: 1670 | Test: 0.0635194182395935 | Test loss: 0.14842066168785095\n",
            "Epoch: 1680 | Test: 0.06317643821239471 | Test loss: 0.14766208827495575\n",
            "Epoch: 1690 | Test: 0.06283293664455414 | Test loss: 0.1468348354101181\n",
            "Epoch: 1700 | Test: 0.062489092350006104 | Test loss: 0.1460762470960617\n",
            "Epoch: 1710 | Test: 0.06214624643325806 | Test loss: 0.14524897933006287\n",
            "Epoch: 1720 | Test: 0.061802566051483154 | Test loss: 0.14442172646522522\n",
            "Epoch: 1730 | Test: 0.06145889312028885 | Test loss: 0.1436631679534912\n",
            "Epoch: 1740 | Test: 0.0611160509288311 | Test loss: 0.14283588528633118\n",
            "Epoch: 1750 | Test: 0.06077219173312187 | Test loss: 0.14200863242149353\n",
            "Epoch: 1760 | Test: 0.06042869761586189 | Test loss: 0.14125005900859833\n",
            "Epoch: 1770 | Test: 0.060085076838731766 | Test loss: 0.14038848876953125\n",
            "Epoch: 1780 | Test: 0.05974218249320984 | Test loss: 0.13962991535663605\n",
            "Epoch: 1790 | Test: 0.0593985989689827 | Test loss: 0.1388026475906372\n",
            "Epoch: 1800 | Test: 0.05905482918024063 | Test loss: 0.1380440890789032\n",
            "Epoch: 1810 | Test: 0.058711983263492584 | Test loss: 0.13721679151058197\n",
            "Epoch: 1820 | Test: 0.058368224650621414 | Test loss: 0.13638953864574432\n",
            "Epoch: 1830 | Test: 0.058024633675813675 | Test loss: 0.13563096523284912\n",
            "Epoch: 1840 | Test: 0.05768175050616264 | Test loss: 0.13480371236801147\n",
            "Epoch: 1850 | Test: 0.05733786150813103 | Test loss: 0.13397642970085144\n",
            "Epoch: 1860 | Test: 0.05699443072080612 | Test loss: 0.13321788609027863\n",
            "Epoch: 1870 | Test: 0.05665137618780136 | Test loss: 0.1323906034231186\n",
            "Epoch: 1880 | Test: 0.056307487189769745 | Test loss: 0.13156335055828094\n",
            "Epoch: 1890 | Test: 0.055964238941669464 | Test loss: 0.13080477714538574\n",
            "Epoch: 1900 | Test: 0.05562101677060127 | Test loss: 0.1299775093793869\n",
            "Epoch: 1910 | Test: 0.05527771636843681 | Test loss: 0.12918460369110107\n",
            "Epoch: 1920 | Test: 0.054933883249759674 | Test loss: 0.12835735082626343\n",
            "Epoch: 1930 | Test: 0.054590366780757904 | Test loss: 0.12759876251220703\n",
            "Epoch: 1940 | Test: 0.054247401654720306 | Test loss: 0.12677152454853058\n",
            "Epoch: 1950 | Test: 0.053903527557849884 | Test loss: 0.12594425678253174\n",
            "Epoch: 1960 | Test: 0.05356016755104065 | Test loss: 0.12518568336963654\n",
            "Epoch: 1970 | Test: 0.053217045962810516 | Test loss: 0.12435843795537949\n",
            "Epoch: 1980 | Test: 0.0528731569647789 | Test loss: 0.12353116273880005\n",
            "Epoch: 1990 | Test: 0.05252997204661369 | Test loss: 0.12277257442474365\n",
            "Epoch: 2000 | Test: 0.05218667909502983 | Test loss: 0.1219453364610672\n",
            "Epoch: 2010 | Test: 0.051842786371707916 | Test loss: 0.12111806869506836\n",
            "Epoch: 2020 | Test: 0.05149977654218674 | Test loss: 0.12035946547985077\n",
            "Epoch: 2030 | Test: 0.051156312227249146 | Test loss: 0.11953222751617432\n",
            "Epoch: 2040 | Test: 0.050812430679798126 | Test loss: 0.11873934417963028\n",
            "Epoch: 2050 | Test: 0.050469182431697845 | Test loss: 0.11791207641363144\n",
            "Epoch: 2060 | Test: 0.050125908106565475 | Test loss: 0.11715348064899445\n",
            "Epoch: 2070 | Test: 0.049782704561948776 | Test loss: 0.1163262352347374\n",
            "Epoch: 2080 | Test: 0.04943882301449776 | Test loss: 0.11549897491931915\n",
            "Epoch: 2090 | Test: 0.04909570887684822 | Test loss: 0.11474039405584335\n",
            "Epoch: 2100 | Test: 0.04875233769416809 | Test loss: 0.11391313374042511\n",
            "Epoch: 2110 | Test: 0.04840845242142677 | Test loss: 0.11308586597442627\n",
            "Epoch: 2120 | Test: 0.048065513372421265 | Test loss: 0.11232730001211166\n",
            "Epoch: 2130 | Test: 0.047721974551677704 | Test loss: 0.11150002479553223\n",
            "Epoch: 2140 | Test: 0.04737816005945206 | Test loss: 0.11074145138263702\n",
            "Epoch: 2150 | Test: 0.04703531414270401 | Test loss: 0.10991419851779938\n",
            "Epoch: 2160 | Test: 0.04669161140918732 | Test loss: 0.10908694565296173\n",
            "Epoch: 2170 | Test: 0.0463479645550251 | Test loss: 0.10832836478948593\n",
            "Epoch: 2180 | Test: 0.046004485338926315 | Test loss: 0.10746679455041885\n",
            "Epoch: 2190 | Test: 0.04566144198179245 | Test loss: 0.10670821368694305\n",
            "Epoch: 2200 | Test: 0.04531800001859665 | Test loss: 0.10588095337152481\n",
            "Epoch: 2210 | Test: 0.04497411102056503 | Test loss: 0.10505368560552597\n",
            "Epoch: 2220 | Test: 0.04463125392794609 | Test loss: 0.10429511219263077\n",
            "Epoch: 2230 | Test: 0.044287633150815964 | Test loss: 0.10346784442663193\n",
            "Epoch: 2240 | Test: 0.04394390061497688 | Test loss: 0.10270927101373672\n",
            "Epoch: 2250 | Test: 0.04360104724764824 | Test loss: 0.10188200324773788\n",
            "Epoch: 2260 | Test: 0.043257273733615875 | Test loss: 0.10105475038290024\n",
            "Epoch: 2270 | Test: 0.04291370138525963 | Test loss: 0.10029617697000504\n",
            "Epoch: 2280 | Test: 0.04257078468799591 | Test loss: 0.0994689017534256\n",
            "Epoch: 2290 | Test: 0.04222690314054489 | Test loss: 0.09864166378974915\n",
            "Epoch: 2300 | Test: 0.04188350588083267 | Test loss: 0.09788306802511215\n",
            "Epoch: 2310 | Test: 0.041540421545505524 | Test loss: 0.09705580770969391\n",
            "Epoch: 2320 | Test: 0.04119697958230972 | Test loss: 0.09626291692256927\n",
            "Epoch: 2330 | Test: 0.04085329547524452 | Test loss: 0.09543566405773163\n",
            "Epoch: 2340 | Test: 0.040509629994630814 | Test loss: 0.09467708319425583\n",
            "Epoch: 2350 | Test: 0.040166791528463364 | Test loss: 0.0938497930765152\n",
            "Epoch: 2360 | Test: 0.039822906255722046 | Test loss: 0.0930224284529686\n",
            "Epoch: 2370 | Test: 0.03947939723730087 | Test loss: 0.09226371347904205\n",
            "Epoch: 2380 | Test: 0.0391363725066185 | Test loss: 0.09143636375665665\n",
            "Epoch: 2390 | Test: 0.03879246488213539 | Test loss: 0.09060899168252945\n",
            "Epoch: 2400 | Test: 0.038449134677648544 | Test loss: 0.08985026925802231\n",
            "Epoch: 2410 | Test: 0.038105934858322144 | Test loss: 0.0890229195356369\n",
            "Epoch: 2420 | Test: 0.037762030959129333 | Test loss: 0.08819553256034851\n",
            "Epoch: 2430 | Test: 0.03741887956857681 | Test loss: 0.08743683248758316\n",
            "Epoch: 2440 | Test: 0.03707549721002579 | Test loss: 0.08660946786403656\n",
            "Epoch: 2450 | Test: 0.03673158958554268 | Test loss: 0.08578209578990936\n",
            "Epoch: 2460 | Test: 0.036388617008924484 | Test loss: 0.08502338081598282\n",
            "Epoch: 2470 | Test: 0.03604506328701973 | Test loss: 0.08419602364301682\n",
            "Epoch: 2480 | Test: 0.03570122644305229 | Test loss: 0.08343730866909027\n",
            "Epoch: 2490 | Test: 0.035358358174562454 | Test loss: 0.08260994404554367\n",
            "Epoch: 2500 | Test: 0.03501463308930397 | Test loss: 0.08178257942199707\n",
            "Epoch: 2510 | Test: 0.03467097133398056 | Test loss: 0.08102386444807053\n",
            "Epoch: 2520 | Test: 0.034328095614910126 | Test loss: 0.08019649982452393\n",
            "Epoch: 2530 | Test: 0.033984191715717316 | Test loss: 0.07936911284923553\n",
            "Epoch: 2540 | Test: 0.03364071249961853 | Test loss: 0.07861043512821198\n",
            "Epoch: 2550 | Test: 0.033297665417194366 | Test loss: 0.07778306305408478\n",
            "Epoch: 2560 | Test: 0.03295375779271126 | Test loss: 0.07695569097995758\n",
            "Epoch: 2570 | Test: 0.0326104573905468 | Test loss: 0.07619697600603104\n",
            "Epoch: 2580 | Test: 0.03226722404360771 | Test loss: 0.07536961138248444\n",
            "Epoch: 2590 | Test: 0.0319233238697052 | Test loss: 0.07454224675893784\n",
            "Epoch: 2600 | Test: 0.03158019110560417 | Test loss: 0.07378353178501129\n",
            "Epoch: 2610 | Test: 0.031236786395311356 | Test loss: 0.07295616716146469\n",
            "Epoch: 2620 | Test: 0.030892882496118546 | Test loss: 0.07212880253791809\n",
            "Epoch: 2630 | Test: 0.03054993413388729 | Test loss: 0.07137009501457214\n",
            "Epoch: 2640 | Test: 0.030206352472305298 | Test loss: 0.07054271548986435\n",
            "Epoch: 2650 | Test: 0.0298625435680151 | Test loss: 0.06978403031826019\n",
            "Epoch: 2660 | Test: 0.02951967716217041 | Test loss: 0.0689566507935524\n",
            "Epoch: 2670 | Test: 0.02917591668665409 | Test loss: 0.0681292787194252\n",
            "Epoch: 2680 | Test: 0.028832275420427322 | Test loss: 0.06737055629491806\n",
            "Epoch: 2690 | Test: 0.028489386662840843 | Test loss: 0.06654320657253265\n",
            "Epoch: 2700 | Test: 0.028145480901002884 | Test loss: 0.06571583449840546\n",
            "Epoch: 2710 | Test: 0.02780202403664589 | Test loss: 0.06495712697505951\n",
            "Epoch: 2720 | Test: 0.027458954602479935 | Test loss: 0.0641297698020935\n",
            "Epoch: 2730 | Test: 0.027115046977996826 | Test loss: 0.06330239772796631\n",
            "Epoch: 2740 | Test: 0.02677176520228386 | Test loss: 0.06254368275403976\n",
            "Epoch: 2750 | Test: 0.02642851509153843 | Test loss: 0.061716336756944656\n",
            "Epoch: 2760 | Test: 0.02608461305499077 | Test loss: 0.060888927429914474\n",
            "Epoch: 2770 | Test: 0.025741511955857277 | Test loss: 0.060130245983600616\n",
            "Epoch: 2780 | Test: 0.02539808116853237 | Test loss: 0.05930287763476372\n",
            "Epoch: 2790 | Test: 0.025054175406694412 | Test loss: 0.05847550556063652\n",
            "Epoch: 2800 | Test: 0.024711258709430695 | Test loss: 0.057716794312000275\n",
            "Epoch: 2810 | Test: 0.024367641657590866 | Test loss: 0.056889425963163376\n",
            "Epoch: 2820 | Test: 0.024023864418268204 | Test loss: 0.056130729615688324\n",
            "Epoch: 2830 | Test: 0.02368099056184292 | Test loss: 0.05530335381627083\n",
            "Epoch: 2840 | Test: 0.02333720587193966 | Test loss: 0.05447598174214363\n",
            "Epoch: 2850 | Test: 0.022993599995970726 | Test loss: 0.05371727794408798\n",
            "Epoch: 2860 | Test: 0.022650673985481262 | Test loss: 0.05288991332054138\n",
            "Epoch: 2870 | Test: 0.022306770086288452 | Test loss: 0.05206253379583359\n",
            "Epoch: 2880 | Test: 0.021963339298963547 | Test loss: 0.051303841173648834\n",
            "Epoch: 2890 | Test: 0.021620238199830055 | Test loss: 0.05047646164894104\n",
            "Epoch: 2900 | Test: 0.021276336163282394 | Test loss: 0.04964910075068474\n",
            "Epoch: 2910 | Test: 0.02093307301402092 | Test loss: 0.048890382051467896\n",
            "Epoch: 2920 | Test: 0.020589804276823997 | Test loss: 0.048063021153211594\n",
            "Epoch: 2930 | Test: 0.020245898514986038 | Test loss: 0.047235649079084396\n",
            "Epoch: 2940 | Test: 0.019902823492884636 | Test loss: 0.04647694155573845\n",
            "Epoch: 2950 | Test: 0.01955937221646309 | Test loss: 0.04564958065748215\n",
            "Epoch: 2960 | Test: 0.01921546645462513 | Test loss: 0.04482220858335495\n",
            "Epoch: 2970 | Test: 0.018872566521167755 | Test loss: 0.044063501060009\n",
            "Epoch: 2980 | Test: 0.018528930842876434 | Test loss: 0.043236155062913895\n",
            "Epoch: 2990 | Test: 0.018185175955295563 | Test loss: 0.042477428913116455\n",
            "Epoch: 3000 | Test: 0.017842309549450874 | Test loss: 0.04165006801486015\n",
            "Epoch: 3010 | Test: 0.017498498782515526 | Test loss: 0.040822695940732956\n",
            "Epoch: 3020 | Test: 0.017154913395643234 | Test loss: 0.04006395861506462\n",
            "Epoch: 3030 | Test: 0.01681196317076683 | Test loss: 0.03923661261796951\n",
            "Epoch: 3040 | Test: 0.01646805927157402 | Test loss: 0.038409244269132614\n",
            "Epoch: 3050 | Test: 0.0161246620118618 | Test loss: 0.037650544196367264\n",
            "Epoch: 3060 | Test: 0.015781531110405922 | Test loss: 0.036823172122240067\n",
            "Epoch: 3070 | Test: 0.015437623485922813 | Test loss: 0.03599580377340317\n",
            "Epoch: 3080 | Test: 0.015094399452209473 | Test loss: 0.03523709252476692\n",
            "Epoch: 3090 | Test: 0.014751088805496693 | Test loss: 0.034409742802381516\n",
            "Epoch: 3100 | Test: 0.014407187700271606 | Test loss: 0.03358235955238342\n",
            "Epoch: 3110 | Test: 0.014064138755202293 | Test loss: 0.032823652029037476\n",
            "Epoch: 3120 | Test: 0.013720658607780933 | Test loss: 0.031996287405490875\n",
            "Epoch: 3130 | Test: 0.013376754708588123 | Test loss: 0.03116891346871853\n",
            "Epoch: 3140 | Test: 0.013033872470259666 | Test loss: 0.030410200357437134\n",
            "Epoch: 3150 | Test: 0.012690221890807152 | Test loss: 0.029582839459180832\n",
            "Epoch: 3160 | Test: 0.012346489354968071 | Test loss: 0.02882412075996399\n",
            "Epoch: 3170 | Test: 0.012003622017800808 | Test loss: 0.027996767312288284\n",
            "Epoch: 3180 | Test: 0.011659789830446243 | Test loss: 0.027169400826096535\n",
            "Epoch: 3190 | Test: 0.01131623238325119 | Test loss: 0.026410680264234543\n",
            "Epoch: 3200 | Test: 0.010973252356052399 | Test loss: 0.02558332122862339\n",
            "Epoch: 3210 | Test: 0.010629348456859589 | Test loss: 0.024755973368883133\n",
            "Epoch: 3220 | Test: 0.01028597541153431 | Test loss: 0.023997247219085693\n",
            "Epoch: 3230 | Test: 0.009942819364368916 | Test loss: 0.023169880732893944\n",
            "Epoch: 3240 | Test: 0.009598915465176105 | Test loss: 0.022342514246702194\n",
            "Epoch: 3250 | Test: 0.009255713783204556 | Test loss: 0.02158377692103386\n",
            "Epoch: 3260 | Test: 0.008912380784749985 | Test loss: 0.020756429061293602\n",
            "Epoch: 3270 | Test: 0.008568478748202324 | Test loss: 0.019929056987166405\n",
            "Epoch: 3280 | Test: 0.008225461468100548 | Test loss: 0.019170355051755905\n",
            "Epoch: 3290 | Test: 0.007881950587034225 | Test loss: 0.018342990428209305\n",
            "Epoch: 3300 | Test: 0.007538061589002609 | Test loss: 0.017584258690476418\n",
            "Epoch: 3310 | Test: 0.00719519704580307 | Test loss: 0.01675691083073616\n",
            "Epoch: 3320 | Test: 0.006851506885141134 | Test loss: 0.015929561108350754\n",
            "Epoch: 3330 | Test: 0.006507802754640579 | Test loss: 0.015170830301940441\n",
            "Epoch: 3340 | Test: 0.006164937280118465 | Test loss: 0.014343464747071266\n",
            "Epoch: 3350 | Test: 0.005821075290441513 | Test loss: 0.013516103848814964\n",
            "Epoch: 3360 | Test: 0.005477549973875284 | Test loss: 0.01275736652314663\n",
            "Epoch: 3370 | Test: 0.005134542938321829 | Test loss: 0.011930018663406372\n",
            "Epoch: 3380 | Test: 0.004790639039129019 | Test loss: 0.011102658696472645\n",
            "Epoch: 3390 | Test: 0.004447287879884243 | Test loss: 0.010343939065933228\n",
            "Epoch: 3400 | Test: 0.004104107618331909 | Test loss: 0.009516584686934948\n",
            "Epoch: 3410 | Test: 0.0037602067459374666 | Test loss: 0.008689219132065773\n",
            "Epoch: 3420 | Test: 0.0034170313738286495 | Test loss: 0.007930499501526356\n",
            "Epoch: 3430 | Test: 0.003073671367019415 | Test loss: 0.0071031274273991585\n",
            "Epoch: 3440 | Test: 0.002729764673858881 | Test loss: 0.006275790743529797\n",
            "Epoch: 3450 | Test: 0.002386773470789194 | Test loss: 0.005517065525054932\n",
            "Epoch: 3460 | Test: 0.0020432353485375643 | Test loss: 0.004689699504524469\n",
            "Epoch: 3470 | Test: 0.0016993805766105652 | Test loss: 0.003930979873985052\n",
            "Epoch: 3480 | Test: 0.0013565116096287966 | Test loss: 0.0031035959254950285\n",
            "Epoch: 3490 | Test: 0.00101279909722507 | Test loss: 0.0022762478329241276\n",
            "Epoch: 3500 | Test: 0.000669122498948127 | Test loss: 0.0015175342559814453\n",
            "Epoch: 3510 | Test: 0.0003262616810388863 | Test loss: 0.0006901741144247353\n",
            "Epoch: 3520 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3530 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3540 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3550 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3560 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3570 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3580 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3590 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3600 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3610 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3620 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3630 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3640 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3650 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3660 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3670 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3680 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3690 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3700 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3710 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3720 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3730 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3740 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3750 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3760 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3770 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3780 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3790 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3800 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3810 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3820 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3830 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3840 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3850 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3860 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3870 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3880 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3890 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3900 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3910 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3920 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3930 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3940 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3950 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3960 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3970 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3980 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 3990 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4000 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4010 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4020 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4030 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4040 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4050 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4060 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4070 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4080 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4090 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4100 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4110 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4120 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4130 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4140 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4150 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4160 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4170 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4180 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4190 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4200 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4210 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4220 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4230 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4240 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4250 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4260 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4270 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4280 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4290 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4300 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4310 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4320 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4330 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4340 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4350 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4360 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4370 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4380 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4390 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4400 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4410 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4420 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4430 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4440 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4450 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4460 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4470 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4480 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4490 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4500 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4510 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4520 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4530 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4540 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4550 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4560 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4570 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4580 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4590 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4600 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4610 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4620 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4630 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4640 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4650 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4660 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4670 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4680 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4690 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4700 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4710 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4720 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4730 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4740 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4750 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4760 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4770 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4780 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4790 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4800 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4810 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4820 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4830 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4840 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4850 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4860 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4870 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4880 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4890 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4900 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4910 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4920 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4930 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4940 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4950 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4960 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4970 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4980 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n",
            "Epoch: 4990 | Test: 0.0008953399956226349 | Test loss: 0.0005240082973614335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "id": "AzqmPFpNxVXc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09d9c5d8-a4b0-4fe4-d654-15b6c2c7a258"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.6999])), ('bias', tensor([0.3010]))])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "  y_preds = model_0(X_test)\n",
        "\n",
        "y_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neSPn5pNrsm1",
        "outputId": "57e08451-f79d-4cb8-d986-ff7b20c5403f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8608],\n",
              "        [0.8748],\n",
              "        [0.8888],\n",
              "        [0.9028],\n",
              "        [0.9168],\n",
              "        [0.9308],\n",
              "        [0.9448],\n",
              "        [0.9588],\n",
              "        [0.9728],\n",
              "        [0.9868]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(predictions = y_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "-oiOiyW3sUnE",
        "outputId": "621eca53-59d5-440e-f9e1-975f57875daf"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss curves\n",
        "epoch_count, loss_values, test_loss_values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LzXM4TrJsaNk",
        "outputId": "1a1bcc88-f29a-463e-bd54-6b59d6ec0aff"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0,\n",
              "  1,\n",
              "  2,\n",
              "  3,\n",
              "  4,\n",
              "  5,\n",
              "  6,\n",
              "  7,\n",
              "  8,\n",
              "  9,\n",
              "  10,\n",
              "  11,\n",
              "  12,\n",
              "  13,\n",
              "  14,\n",
              "  15,\n",
              "  16,\n",
              "  17,\n",
              "  18,\n",
              "  19,\n",
              "  20,\n",
              "  21,\n",
              "  22,\n",
              "  23,\n",
              "  24,\n",
              "  25,\n",
              "  26,\n",
              "  27,\n",
              "  28,\n",
              "  29,\n",
              "  30,\n",
              "  31,\n",
              "  32,\n",
              "  33,\n",
              "  34,\n",
              "  35,\n",
              "  36,\n",
              "  37,\n",
              "  38,\n",
              "  39,\n",
              "  40,\n",
              "  41,\n",
              "  42,\n",
              "  43,\n",
              "  44,\n",
              "  45,\n",
              "  46,\n",
              "  47,\n",
              "  48,\n",
              "  49,\n",
              "  50,\n",
              "  51,\n",
              "  52,\n",
              "  53,\n",
              "  54,\n",
              "  55,\n",
              "  56,\n",
              "  57,\n",
              "  58,\n",
              "  59,\n",
              "  60,\n",
              "  61,\n",
              "  62,\n",
              "  63,\n",
              "  64,\n",
              "  65,\n",
              "  66,\n",
              "  67,\n",
              "  68,\n",
              "  69,\n",
              "  70,\n",
              "  71,\n",
              "  72,\n",
              "  73,\n",
              "  74,\n",
              "  75,\n",
              "  76,\n",
              "  77,\n",
              "  78,\n",
              "  79,\n",
              "  80,\n",
              "  81,\n",
              "  82,\n",
              "  83,\n",
              "  84,\n",
              "  85,\n",
              "  86,\n",
              "  87,\n",
              "  88,\n",
              "  89,\n",
              "  90,\n",
              "  91,\n",
              "  92,\n",
              "  93,\n",
              "  94,\n",
              "  95,\n",
              "  96,\n",
              "  97,\n",
              "  98,\n",
              "  99,\n",
              "  100,\n",
              "  101,\n",
              "  102,\n",
              "  103,\n",
              "  104,\n",
              "  105,\n",
              "  106,\n",
              "  107,\n",
              "  108,\n",
              "  109,\n",
              "  110,\n",
              "  111,\n",
              "  112,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  117,\n",
              "  118,\n",
              "  119,\n",
              "  120,\n",
              "  121,\n",
              "  122,\n",
              "  123,\n",
              "  124,\n",
              "  125,\n",
              "  126,\n",
              "  127,\n",
              "  128,\n",
              "  129,\n",
              "  130,\n",
              "  131,\n",
              "  132,\n",
              "  133,\n",
              "  134,\n",
              "  135,\n",
              "  136,\n",
              "  137,\n",
              "  138,\n",
              "  139,\n",
              "  140,\n",
              "  141,\n",
              "  142,\n",
              "  143,\n",
              "  144,\n",
              "  145,\n",
              "  146,\n",
              "  147,\n",
              "  148,\n",
              "  149,\n",
              "  150,\n",
              "  151,\n",
              "  152,\n",
              "  153,\n",
              "  154,\n",
              "  155,\n",
              "  156,\n",
              "  157,\n",
              "  158,\n",
              "  159,\n",
              "  160,\n",
              "  161,\n",
              "  162,\n",
              "  163,\n",
              "  164,\n",
              "  165,\n",
              "  166,\n",
              "  167,\n",
              "  168,\n",
              "  169,\n",
              "  170,\n",
              "  171,\n",
              "  172,\n",
              "  173,\n",
              "  174,\n",
              "  175,\n",
              "  176,\n",
              "  177,\n",
              "  178,\n",
              "  179,\n",
              "  180,\n",
              "  181,\n",
              "  182,\n",
              "  183,\n",
              "  184,\n",
              "  185,\n",
              "  186,\n",
              "  187,\n",
              "  188,\n",
              "  189,\n",
              "  190,\n",
              "  191,\n",
              "  192,\n",
              "  193,\n",
              "  194,\n",
              "  195,\n",
              "  196,\n",
              "  197,\n",
              "  198,\n",
              "  199,\n",
              "  200,\n",
              "  201,\n",
              "  202,\n",
              "  203,\n",
              "  204,\n",
              "  205,\n",
              "  206,\n",
              "  207,\n",
              "  208,\n",
              "  209,\n",
              "  210,\n",
              "  211,\n",
              "  212,\n",
              "  213,\n",
              "  214,\n",
              "  215,\n",
              "  216,\n",
              "  217,\n",
              "  218,\n",
              "  219,\n",
              "  220,\n",
              "  221,\n",
              "  222,\n",
              "  223,\n",
              "  224,\n",
              "  225,\n",
              "  226,\n",
              "  227,\n",
              "  228,\n",
              "  229,\n",
              "  230,\n",
              "  231,\n",
              "  232,\n",
              "  233,\n",
              "  234,\n",
              "  235,\n",
              "  236,\n",
              "  237,\n",
              "  238,\n",
              "  239,\n",
              "  240,\n",
              "  241,\n",
              "  242,\n",
              "  243,\n",
              "  244,\n",
              "  245,\n",
              "  246,\n",
              "  247,\n",
              "  248,\n",
              "  249,\n",
              "  250,\n",
              "  251,\n",
              "  252,\n",
              "  253,\n",
              "  254,\n",
              "  255,\n",
              "  256,\n",
              "  257,\n",
              "  258,\n",
              "  259,\n",
              "  260,\n",
              "  261,\n",
              "  262,\n",
              "  263,\n",
              "  264,\n",
              "  265,\n",
              "  266,\n",
              "  267,\n",
              "  268,\n",
              "  269,\n",
              "  270,\n",
              "  271,\n",
              "  272,\n",
              "  273,\n",
              "  274,\n",
              "  275,\n",
              "  276,\n",
              "  277,\n",
              "  278,\n",
              "  279,\n",
              "  280,\n",
              "  281,\n",
              "  282,\n",
              "  283,\n",
              "  284,\n",
              "  285,\n",
              "  286,\n",
              "  287,\n",
              "  288,\n",
              "  289,\n",
              "  290,\n",
              "  291,\n",
              "  292,\n",
              "  293,\n",
              "  294,\n",
              "  295,\n",
              "  296,\n",
              "  297,\n",
              "  298,\n",
              "  299,\n",
              "  300,\n",
              "  301,\n",
              "  302,\n",
              "  303,\n",
              "  304,\n",
              "  305,\n",
              "  306,\n",
              "  307,\n",
              "  308,\n",
              "  309,\n",
              "  310,\n",
              "  311,\n",
              "  312,\n",
              "  313,\n",
              "  314,\n",
              "  315,\n",
              "  316,\n",
              "  317,\n",
              "  318,\n",
              "  319,\n",
              "  320,\n",
              "  321,\n",
              "  322,\n",
              "  323,\n",
              "  324,\n",
              "  325,\n",
              "  326,\n",
              "  327,\n",
              "  328,\n",
              "  329,\n",
              "  330,\n",
              "  331,\n",
              "  332,\n",
              "  333,\n",
              "  334,\n",
              "  335,\n",
              "  336,\n",
              "  337,\n",
              "  338,\n",
              "  339,\n",
              "  340,\n",
              "  341,\n",
              "  342,\n",
              "  343,\n",
              "  344,\n",
              "  345,\n",
              "  346,\n",
              "  347,\n",
              "  348,\n",
              "  349,\n",
              "  350,\n",
              "  351,\n",
              "  352,\n",
              "  353,\n",
              "  354,\n",
              "  355,\n",
              "  356,\n",
              "  357,\n",
              "  358,\n",
              "  359,\n",
              "  360,\n",
              "  361,\n",
              "  362,\n",
              "  363,\n",
              "  364,\n",
              "  365,\n",
              "  366,\n",
              "  367,\n",
              "  368,\n",
              "  369,\n",
              "  370,\n",
              "  371,\n",
              "  372,\n",
              "  373,\n",
              "  374,\n",
              "  375,\n",
              "  376,\n",
              "  377,\n",
              "  378,\n",
              "  379,\n",
              "  380,\n",
              "  381,\n",
              "  382,\n",
              "  383,\n",
              "  384,\n",
              "  385,\n",
              "  386,\n",
              "  387,\n",
              "  388,\n",
              "  389,\n",
              "  390,\n",
              "  391,\n",
              "  392,\n",
              "  393,\n",
              "  394,\n",
              "  395,\n",
              "  396,\n",
              "  397,\n",
              "  398,\n",
              "  399,\n",
              "  400,\n",
              "  401,\n",
              "  402,\n",
              "  403,\n",
              "  404,\n",
              "  405,\n",
              "  406,\n",
              "  407,\n",
              "  408,\n",
              "  409,\n",
              "  410,\n",
              "  411,\n",
              "  412,\n",
              "  413,\n",
              "  414,\n",
              "  415,\n",
              "  416,\n",
              "  417,\n",
              "  418,\n",
              "  419,\n",
              "  420,\n",
              "  421,\n",
              "  422,\n",
              "  423,\n",
              "  424,\n",
              "  425,\n",
              "  426,\n",
              "  427,\n",
              "  428,\n",
              "  429,\n",
              "  430,\n",
              "  431,\n",
              "  432,\n",
              "  433,\n",
              "  434,\n",
              "  435,\n",
              "  436,\n",
              "  437,\n",
              "  438,\n",
              "  439,\n",
              "  440,\n",
              "  441,\n",
              "  442,\n",
              "  443,\n",
              "  444,\n",
              "  445,\n",
              "  446,\n",
              "  447,\n",
              "  448,\n",
              "  449,\n",
              "  450,\n",
              "  451,\n",
              "  452,\n",
              "  453,\n",
              "  454,\n",
              "  455,\n",
              "  456,\n",
              "  457,\n",
              "  458,\n",
              "  459,\n",
              "  460,\n",
              "  461,\n",
              "  462,\n",
              "  463,\n",
              "  464,\n",
              "  465,\n",
              "  466,\n",
              "  467,\n",
              "  468,\n",
              "  469,\n",
              "  470,\n",
              "  471,\n",
              "  472,\n",
              "  473,\n",
              "  474,\n",
              "  475,\n",
              "  476,\n",
              "  477,\n",
              "  478,\n",
              "  479,\n",
              "  480,\n",
              "  481,\n",
              "  482,\n",
              "  483,\n",
              "  484,\n",
              "  485,\n",
              "  486,\n",
              "  487,\n",
              "  488,\n",
              "  489,\n",
              "  490,\n",
              "  491,\n",
              "  492,\n",
              "  493,\n",
              "  494,\n",
              "  495,\n",
              "  496,\n",
              "  497,\n",
              "  498,\n",
              "  499,\n",
              "  500,\n",
              "  501,\n",
              "  502,\n",
              "  503,\n",
              "  504,\n",
              "  505,\n",
              "  506,\n",
              "  507,\n",
              "  508,\n",
              "  509,\n",
              "  510,\n",
              "  511,\n",
              "  512,\n",
              "  513,\n",
              "  514,\n",
              "  515,\n",
              "  516,\n",
              "  517,\n",
              "  518,\n",
              "  519,\n",
              "  520,\n",
              "  521,\n",
              "  522,\n",
              "  523,\n",
              "  524,\n",
              "  525,\n",
              "  526,\n",
              "  527,\n",
              "  528,\n",
              "  529,\n",
              "  530,\n",
              "  531,\n",
              "  532,\n",
              "  533,\n",
              "  534,\n",
              "  535,\n",
              "  536,\n",
              "  537,\n",
              "  538,\n",
              "  539,\n",
              "  540,\n",
              "  541,\n",
              "  542,\n",
              "  543,\n",
              "  544,\n",
              "  545,\n",
              "  546,\n",
              "  547,\n",
              "  548,\n",
              "  549,\n",
              "  550,\n",
              "  551,\n",
              "  552,\n",
              "  553,\n",
              "  554,\n",
              "  555,\n",
              "  556,\n",
              "  557,\n",
              "  558,\n",
              "  559,\n",
              "  560,\n",
              "  561,\n",
              "  562,\n",
              "  563,\n",
              "  564,\n",
              "  565,\n",
              "  566,\n",
              "  567,\n",
              "  568,\n",
              "  569,\n",
              "  570,\n",
              "  571,\n",
              "  572,\n",
              "  573,\n",
              "  574,\n",
              "  575,\n",
              "  576,\n",
              "  577,\n",
              "  578,\n",
              "  579,\n",
              "  580,\n",
              "  581,\n",
              "  582,\n",
              "  583,\n",
              "  584,\n",
              "  585,\n",
              "  586,\n",
              "  587,\n",
              "  588,\n",
              "  589,\n",
              "  590,\n",
              "  591,\n",
              "  592,\n",
              "  593,\n",
              "  594,\n",
              "  595,\n",
              "  596,\n",
              "  597,\n",
              "  598,\n",
              "  599,\n",
              "  600,\n",
              "  601,\n",
              "  602,\n",
              "  603,\n",
              "  604,\n",
              "  605,\n",
              "  606,\n",
              "  607,\n",
              "  608,\n",
              "  609,\n",
              "  610,\n",
              "  611,\n",
              "  612,\n",
              "  613,\n",
              "  614,\n",
              "  615,\n",
              "  616,\n",
              "  617,\n",
              "  618,\n",
              "  619,\n",
              "  620,\n",
              "  621,\n",
              "  622,\n",
              "  623,\n",
              "  624,\n",
              "  625,\n",
              "  626,\n",
              "  627,\n",
              "  628,\n",
              "  629,\n",
              "  630,\n",
              "  631,\n",
              "  632,\n",
              "  633,\n",
              "  634,\n",
              "  635,\n",
              "  636,\n",
              "  637,\n",
              "  638,\n",
              "  639,\n",
              "  640,\n",
              "  641,\n",
              "  642,\n",
              "  643,\n",
              "  644,\n",
              "  645,\n",
              "  646,\n",
              "  647,\n",
              "  648,\n",
              "  649,\n",
              "  650,\n",
              "  651,\n",
              "  652,\n",
              "  653,\n",
              "  654,\n",
              "  655,\n",
              "  656,\n",
              "  657,\n",
              "  658,\n",
              "  659,\n",
              "  660,\n",
              "  661,\n",
              "  662,\n",
              "  663,\n",
              "  664,\n",
              "  665,\n",
              "  666,\n",
              "  667,\n",
              "  668,\n",
              "  669,\n",
              "  670,\n",
              "  671,\n",
              "  672,\n",
              "  673,\n",
              "  674,\n",
              "  675,\n",
              "  676,\n",
              "  677,\n",
              "  678,\n",
              "  679,\n",
              "  680,\n",
              "  681,\n",
              "  682,\n",
              "  683,\n",
              "  684,\n",
              "  685,\n",
              "  686,\n",
              "  687,\n",
              "  688,\n",
              "  689,\n",
              "  690,\n",
              "  691,\n",
              "  692,\n",
              "  693,\n",
              "  694,\n",
              "  695,\n",
              "  696,\n",
              "  697,\n",
              "  698,\n",
              "  699,\n",
              "  700,\n",
              "  701,\n",
              "  702,\n",
              "  703,\n",
              "  704,\n",
              "  705,\n",
              "  706,\n",
              "  707,\n",
              "  708,\n",
              "  709,\n",
              "  710,\n",
              "  711,\n",
              "  712,\n",
              "  713,\n",
              "  714,\n",
              "  715,\n",
              "  716,\n",
              "  717,\n",
              "  718,\n",
              "  719,\n",
              "  720,\n",
              "  721,\n",
              "  722,\n",
              "  723,\n",
              "  724,\n",
              "  725,\n",
              "  726,\n",
              "  727,\n",
              "  728,\n",
              "  729,\n",
              "  730,\n",
              "  731,\n",
              "  732,\n",
              "  733,\n",
              "  734,\n",
              "  735,\n",
              "  736,\n",
              "  737,\n",
              "  738,\n",
              "  739,\n",
              "  740,\n",
              "  741,\n",
              "  742,\n",
              "  743,\n",
              "  744,\n",
              "  745,\n",
              "  746,\n",
              "  747,\n",
              "  748,\n",
              "  749,\n",
              "  750,\n",
              "  751,\n",
              "  752,\n",
              "  753,\n",
              "  754,\n",
              "  755,\n",
              "  756,\n",
              "  757,\n",
              "  758,\n",
              "  759,\n",
              "  760,\n",
              "  761,\n",
              "  762,\n",
              "  763,\n",
              "  764,\n",
              "  765,\n",
              "  766,\n",
              "  767,\n",
              "  768,\n",
              "  769,\n",
              "  770,\n",
              "  771,\n",
              "  772,\n",
              "  773,\n",
              "  774,\n",
              "  775,\n",
              "  776,\n",
              "  777,\n",
              "  778,\n",
              "  779,\n",
              "  780,\n",
              "  781,\n",
              "  782,\n",
              "  783,\n",
              "  784,\n",
              "  785,\n",
              "  786,\n",
              "  787,\n",
              "  788,\n",
              "  789,\n",
              "  790,\n",
              "  791,\n",
              "  792,\n",
              "  793,\n",
              "  794,\n",
              "  795,\n",
              "  796,\n",
              "  797,\n",
              "  798,\n",
              "  799,\n",
              "  800,\n",
              "  801,\n",
              "  802,\n",
              "  803,\n",
              "  804,\n",
              "  805,\n",
              "  806,\n",
              "  807,\n",
              "  808,\n",
              "  809,\n",
              "  810,\n",
              "  811,\n",
              "  812,\n",
              "  813,\n",
              "  814,\n",
              "  815,\n",
              "  816,\n",
              "  817,\n",
              "  818,\n",
              "  819,\n",
              "  820,\n",
              "  821,\n",
              "  822,\n",
              "  823,\n",
              "  824,\n",
              "  825,\n",
              "  826,\n",
              "  827,\n",
              "  828,\n",
              "  829,\n",
              "  830,\n",
              "  831,\n",
              "  832,\n",
              "  833,\n",
              "  834,\n",
              "  835,\n",
              "  836,\n",
              "  837,\n",
              "  838,\n",
              "  839,\n",
              "  840,\n",
              "  841,\n",
              "  842,\n",
              "  843,\n",
              "  844,\n",
              "  845,\n",
              "  846,\n",
              "  847,\n",
              "  848,\n",
              "  849,\n",
              "  850,\n",
              "  851,\n",
              "  852,\n",
              "  853,\n",
              "  854,\n",
              "  855,\n",
              "  856,\n",
              "  857,\n",
              "  858,\n",
              "  859,\n",
              "  860,\n",
              "  861,\n",
              "  862,\n",
              "  863,\n",
              "  864,\n",
              "  865,\n",
              "  866,\n",
              "  867,\n",
              "  868,\n",
              "  869,\n",
              "  870,\n",
              "  871,\n",
              "  872,\n",
              "  873,\n",
              "  874,\n",
              "  875,\n",
              "  876,\n",
              "  877,\n",
              "  878,\n",
              "  879,\n",
              "  880,\n",
              "  881,\n",
              "  882,\n",
              "  883,\n",
              "  884,\n",
              "  885,\n",
              "  886,\n",
              "  887,\n",
              "  888,\n",
              "  889,\n",
              "  890,\n",
              "  891,\n",
              "  892,\n",
              "  893,\n",
              "  894,\n",
              "  895,\n",
              "  896,\n",
              "  897,\n",
              "  898,\n",
              "  899,\n",
              "  900,\n",
              "  901,\n",
              "  902,\n",
              "  903,\n",
              "  904,\n",
              "  905,\n",
              "  906,\n",
              "  907,\n",
              "  908,\n",
              "  909,\n",
              "  910,\n",
              "  911,\n",
              "  912,\n",
              "  913,\n",
              "  914,\n",
              "  915,\n",
              "  916,\n",
              "  917,\n",
              "  918,\n",
              "  919,\n",
              "  920,\n",
              "  921,\n",
              "  922,\n",
              "  923,\n",
              "  924,\n",
              "  925,\n",
              "  926,\n",
              "  927,\n",
              "  928,\n",
              "  929,\n",
              "  930,\n",
              "  931,\n",
              "  932,\n",
              "  933,\n",
              "  934,\n",
              "  935,\n",
              "  936,\n",
              "  937,\n",
              "  938,\n",
              "  939,\n",
              "  940,\n",
              "  941,\n",
              "  942,\n",
              "  943,\n",
              "  944,\n",
              "  945,\n",
              "  946,\n",
              "  947,\n",
              "  948,\n",
              "  949,\n",
              "  950,\n",
              "  951,\n",
              "  952,\n",
              "  953,\n",
              "  954,\n",
              "  955,\n",
              "  956,\n",
              "  957,\n",
              "  958,\n",
              "  959,\n",
              "  960,\n",
              "  961,\n",
              "  962,\n",
              "  963,\n",
              "  964,\n",
              "  965,\n",
              "  966,\n",
              "  967,\n",
              "  968,\n",
              "  969,\n",
              "  970,\n",
              "  971,\n",
              "  972,\n",
              "  973,\n",
              "  974,\n",
              "  975,\n",
              "  976,\n",
              "  977,\n",
              "  978,\n",
              "  979,\n",
              "  980,\n",
              "  981,\n",
              "  982,\n",
              "  983,\n",
              "  984,\n",
              "  985,\n",
              "  986,\n",
              "  987,\n",
              "  988,\n",
              "  989,\n",
              "  990,\n",
              "  991,\n",
              "  992,\n",
              "  993,\n",
              "  994,\n",
              "  995,\n",
              "  996,\n",
              "  997,\n",
              "  998,\n",
              "  999,\n",
              "  ...],\n",
              " [tensor(0.1753, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1748, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1743, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1738, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1733, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1729, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1725, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1720, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1716, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1712, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1707, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1703, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1699, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1695, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1690, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1686, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1682, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1678, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1673, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1669, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1665, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1660, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1656, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1652, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1648, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1643, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1640, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1636, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1632, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1629, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1625, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1621, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1618, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1614, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1610, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1607, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1603, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1599, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1595, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1592, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1588, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1584, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1581, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1577, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1573, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1570, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1566, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1562, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1559, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1556, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1552, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1549, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1546, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1543, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1540, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1537, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1534, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1530, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1527, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1524, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1521, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1518, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1515, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1512, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1508, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1505, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1502, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1499, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1496, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1493, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1490, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1486, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1483, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1481, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1478, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1475, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1473, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1470, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1467, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1465, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1462, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1460, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1457, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1454, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1452, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1449, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1446, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1444, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1441, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1438, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1436, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1433, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1430, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1428, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1425, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1422, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1420, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1417, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1415, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1412, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1410, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1408, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1406, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1404, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1401, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1399, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1397, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1395, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1393, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1390, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1388, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1386, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1384, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1382, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1379, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1377, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1375, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1373, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1371, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1368, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1366, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1364, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1362, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1360, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1357, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1355, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1353, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1351, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1349, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1347, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1346, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1344, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1342, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1340, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1338, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1337, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1335, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1333, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1331, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1329, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1328, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1326, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1324, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1322, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1320, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1319, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1317, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1315, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1313, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1311, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1310, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1308, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1306, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1304, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1302, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1301, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1299, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1297, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1295, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1294, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1292, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1291, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1289, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1288, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1286, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1285, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1284, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1282, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1281, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1279, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1278, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1276, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1275, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1273, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1272, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1271, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1269, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1268, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1266, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1265, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1263, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1262, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1261, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1259, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1258, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1256, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1255, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1253, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1252, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1250, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1249, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1248, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1246, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1245, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1243, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1242, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1241, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1240, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1239, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1238, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1236, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1235, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1234, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1233, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1232, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1231, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1230, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1228, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1227, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1226, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1225, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1224, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1223, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1222, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1221, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1219, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1218, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1217, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1216, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1215, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1214, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1213, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1212, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1210, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1209, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1208, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1207, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1206, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1205, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1204, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1203, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1201, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1200, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1199, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1198, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1197, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1196, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1195, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1194, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1193, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1192, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1191, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1191, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1190, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1189, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1188, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1187, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1186, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1185, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1185, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1184, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1183, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1182, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1181, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1180, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1179, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1178, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1178, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1177, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1176, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1175, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1174, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1173, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1172, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1171, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1171, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1170, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1169, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1168, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1167, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1166, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1165, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1165, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1164, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1163, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1162, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1161, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1160, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1159, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1158, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1158, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1157, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1156, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1155, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1154, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1153, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1153, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1152, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1151, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1151, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1150, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1149, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1149, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1148, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1147, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1147, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1146, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1145, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1145, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1144, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1143, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1143, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1142, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1141, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1141, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1140, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1139, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1139, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1138, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1137, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1137, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1136, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1135, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1135, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1134, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1133, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1133, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1132, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1131, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1131, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1130, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1129, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1129, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1128, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1127, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1127, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1126, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1125, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1125, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1124, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1123, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1123, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1122, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1122, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1121, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1120, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1120, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1119, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1118, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1118, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1117, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1116, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1116, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1115, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1114, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1114, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1113, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1112, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1112, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1111, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1111, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1110, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1110, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1109, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1109, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1108, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1108, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1107, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1107, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1106, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1106, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1105, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1105, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1104, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1104, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1103, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1103, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1102, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1102, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1101, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1101, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1100, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1100, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1099, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1099, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1098, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1098, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1097, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1097, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1096, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1096, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1095, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1095, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1094, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1094, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1093, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1093, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1092, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1092, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1091, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1091, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1090, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1090, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1089, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1089, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1088, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1088, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1087, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1087, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1086, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1086, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1085, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1085, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1084, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1084, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1083, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1083, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1082, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1082, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1081, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1081, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1080, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1080, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1079, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1079, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1078, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1078, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1077, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1077, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1076, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1076, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1075, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1075, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1074, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1074, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1073, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1073, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1072, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1072, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1071, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1071, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1070, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1070, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1070, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1069, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1069, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1068, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1068, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1068, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1067, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1067, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1066, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1066, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1066, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1065, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1065, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1064, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1064, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1064, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1063, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1063, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1062, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1062, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1062, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1061, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1061, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1060, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1060, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1060, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1059, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1059, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1058, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1058, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1058, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1057, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1057, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1056, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1056, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1056, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1055, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1055, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1054, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1054, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1054, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1053, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1053, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1052, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1052, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1052, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1051, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1051, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1050, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1050, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1050, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1049, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1049, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1048, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1048, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1048, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1047, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1047, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1046, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1046, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1046, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1045, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1045, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1044, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1044, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1044, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1043, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1043, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1042, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1042, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1042, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1041, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1041, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1040, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1040, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1040, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1039, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1039, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1038, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1038, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1038, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1037, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1037, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1036, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1036, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1036, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1035, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1035, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1034, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1034, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1034, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1033, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1033, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1032, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1032, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1032, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1031, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1031, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1030, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1030, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1030, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1029, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1029, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1028, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1028, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1028, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1027, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1027, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1026, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1026, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1026, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1025, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1025, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1024, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1024, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1024, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1023, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1023, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1022, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1022, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1022, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1021, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1021, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1020, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1020, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1020, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1019, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1019, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1018, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1018, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1018, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1017, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1017, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1017, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1016, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1016, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1015, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1015, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1015, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1014, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1014, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1014, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1013, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1013, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1013, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1012, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1012, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1012, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1011, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1011, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1011, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1010, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1010, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1010, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1009, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1009, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1009, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1008, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1008, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1007, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1007, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1007, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1006, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1006, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1006, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1005, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1005, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1005, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1004, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1004, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1004, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1003, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1003, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1003, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1002, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1002, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1002, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1001, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1001, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1000, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1000, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1000, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0999, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0999, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0999, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0998, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0998, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0998, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0997, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0997, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0997, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0996, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0996, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0996, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0995, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0995, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0995, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0994, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0994, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0994, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0993, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0993, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0992, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0992, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0992, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0991, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0991, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0991, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0990, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0990, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0990, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0989, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0989, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0989, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0988, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0988, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0988, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0987, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0987, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0987, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0986, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0986, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0985, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0985, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0985, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0984, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0984, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0984, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0983, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0983, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0983, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0982, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0982, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0982, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0981, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0981, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0981, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0980, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0980, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0980, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0979, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0979, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0978, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0978, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0978, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0977, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0977, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0977, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0976, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0976, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0976, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0975, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0975, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0975, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0974, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0974, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0974, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0973, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0973, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0973, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0972, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0972, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0972, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0971, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0971, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0970, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0970, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0970, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0969, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0969, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0969, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0968, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0968, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0968, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0967, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0967, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0967, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0966, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0966, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0966, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0965, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0965, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0965, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0964, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0964, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0963, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0963, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0963, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0962, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0962, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0962, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0961, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0961, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0961, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0960, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0960, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0960, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0959, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0959, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0959, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0958, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0958, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0958, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0957, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0957, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0957, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0956, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0956, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0955, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0955, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0955, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0954, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0954, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0954, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0953, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0953, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0953, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0952, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0952, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0952, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0951, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0951, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0951, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0950, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0950, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0950, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0949, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0949, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0948, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0948, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0948, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0947, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0947, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0947, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0946, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0946, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0946, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0945, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0945, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0945, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0944, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0944, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0944, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0943, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0943, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0943, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0942, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0942, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0941, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0941, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0941, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0940, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0940, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0940, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0939, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0939, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0939, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0938, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0938, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0938, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0937, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0937, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0937, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0936, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0936, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0936, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0935, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0935, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0935, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0934, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0934, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0933, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0933, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0933, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0932, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0932, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0932, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0931, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0931, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0931, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0930, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0930, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0930, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0929, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0929, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0929, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0928, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0928, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0928, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0927, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0927, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0926, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0926, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0926, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0925, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0925, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0925, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0924, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0924, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0924, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0923, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0923, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0923, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0922, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0922, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0922, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0921, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0921, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0921, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0920, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0920, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0920, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0919, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0919, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0918, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0918, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0918, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0917, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0917, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0917, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0916, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0916, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0916, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0915, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0915, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0915, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0914, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0914, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0914, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0913, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0913, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0913, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0912, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0912, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0911, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0911, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0911, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0910, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0910, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0910, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0909, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0909, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0909, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0908, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0908, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0908, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0907, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0907, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0907, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0906, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0906, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0906, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0905, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0905, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0905, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0904, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0904, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0903, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0903, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0903, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0902, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0902, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0902, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0901, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0901, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0901, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0900, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0900, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0900, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0899, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0899, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0899, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0898, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0898, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0898, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0897, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0897, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0896, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0896, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0896, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0895, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0895, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0895, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0894, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0894, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0894, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0893, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0893, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0893, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0892, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0892, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0892, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0891, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0891, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0891, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0890, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0890, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0889, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0889, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0889, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0888, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0888, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0888, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0887, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0887, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0887, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0886, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0886, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0886, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0885, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0885, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0885, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0884, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0884, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0884, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0883, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0883, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0883, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0882, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0882, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0881, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0881, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0881, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0880, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0880, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0880, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0879, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0879, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0879, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0878, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0878, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0878, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0877, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0877, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0877, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0876, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0876, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0876, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0875, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0875, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0875, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0874, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0874, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0874, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0873, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0873, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0873, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0872, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0872, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0871, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0871, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0871, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0870, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0870, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0870, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0869, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0869, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0869, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0868, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0868, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0868, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0867, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0867, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0867, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0866, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0866, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0866, grad_fn=<MeanBackward0>),\n",
              "  ...],\n",
              " [tensor(0.4786),\n",
              "  tensor(0.4777),\n",
              "  tensor(0.4768),\n",
              "  tensor(0.4759),\n",
              "  tensor(0.4750),\n",
              "  tensor(0.4741),\n",
              "  tensor(0.4733),\n",
              "  tensor(0.4724),\n",
              "  tensor(0.4715),\n",
              "  tensor(0.4707),\n",
              "  tensor(0.4698),\n",
              "  tensor(0.4689),\n",
              "  tensor(0.4681),\n",
              "  tensor(0.4672),\n",
              "  tensor(0.4663),\n",
              "  tensor(0.4655),\n",
              "  tensor(0.4646),\n",
              "  tensor(0.4637),\n",
              "  tensor(0.4629),\n",
              "  tensor(0.4620),\n",
              "  tensor(0.4611),\n",
              "  tensor(0.4603),\n",
              "  tensor(0.4594),\n",
              "  tensor(0.4586),\n",
              "  tensor(0.4577),\n",
              "  tensor(0.4569),\n",
              "  tensor(0.4561),\n",
              "  tensor(0.4553),\n",
              "  tensor(0.4545),\n",
              "  tensor(0.4537),\n",
              "  tensor(0.4528),\n",
              "  tensor(0.4520),\n",
              "  tensor(0.4512),\n",
              "  tensor(0.4504),\n",
              "  tensor(0.4496),\n",
              "  tensor(0.4488),\n",
              "  tensor(0.4480),\n",
              "  tensor(0.4472),\n",
              "  tensor(0.4464),\n",
              "  tensor(0.4456),\n",
              "  tensor(0.4448),\n",
              "  tensor(0.4440),\n",
              "  tensor(0.4432),\n",
              "  tensor(0.4424),\n",
              "  tensor(0.4415),\n",
              "  tensor(0.4407),\n",
              "  tensor(0.4399),\n",
              "  tensor(0.4391),\n",
              "  tensor(0.4384),\n",
              "  tensor(0.4376),\n",
              "  tensor(0.4369),\n",
              "  tensor(0.4361),\n",
              "  tensor(0.4354),\n",
              "  tensor(0.4346),\n",
              "  tensor(0.4339),\n",
              "  tensor(0.4331),\n",
              "  tensor(0.4324),\n",
              "  tensor(0.4316),\n",
              "  tensor(0.4309),\n",
              "  tensor(0.4301),\n",
              "  tensor(0.4294),\n",
              "  tensor(0.4287),\n",
              "  tensor(0.4279),\n",
              "  tensor(0.4272),\n",
              "  tensor(0.4264),\n",
              "  tensor(0.4257),\n",
              "  tensor(0.4249),\n",
              "  tensor(0.4242),\n",
              "  tensor(0.4234),\n",
              "  tensor(0.4227),\n",
              "  tensor(0.4219),\n",
              "  tensor(0.4212),\n",
              "  tensor(0.4205),\n",
              "  tensor(0.4198),\n",
              "  tensor(0.4191),\n",
              "  tensor(0.4184),\n",
              "  tensor(0.4177),\n",
              "  tensor(0.4170),\n",
              "  tensor(0.4164),\n",
              "  tensor(0.4157),\n",
              "  tensor(0.4150),\n",
              "  tensor(0.4143),\n",
              "  tensor(0.4136),\n",
              "  tensor(0.4129),\n",
              "  tensor(0.4122),\n",
              "  tensor(0.4115),\n",
              "  tensor(0.4108),\n",
              "  tensor(0.4102),\n",
              "  tensor(0.4095),\n",
              "  tensor(0.4088),\n",
              "  tensor(0.4081),\n",
              "  tensor(0.4074),\n",
              "  tensor(0.4067),\n",
              "  tensor(0.4060),\n",
              "  tensor(0.4053),\n",
              "  tensor(0.4046),\n",
              "  tensor(0.4040),\n",
              "  tensor(0.4033),\n",
              "  tensor(0.4026),\n",
              "  tensor(0.4020),\n",
              "  tensor(0.4014),\n",
              "  tensor(0.4008),\n",
              "  tensor(0.4001),\n",
              "  tensor(0.3995),\n",
              "  tensor(0.3989),\n",
              "  tensor(0.3982),\n",
              "  tensor(0.3976),\n",
              "  tensor(0.3970),\n",
              "  tensor(0.3964),\n",
              "  tensor(0.3957),\n",
              "  tensor(0.3951),\n",
              "  tensor(0.3945),\n",
              "  tensor(0.3939),\n",
              "  tensor(0.3932),\n",
              "  tensor(0.3926),\n",
              "  tensor(0.3920),\n",
              "  tensor(0.3913),\n",
              "  tensor(0.3907),\n",
              "  tensor(0.3901),\n",
              "  tensor(0.3895),\n",
              "  tensor(0.3888),\n",
              "  tensor(0.3882),\n",
              "  tensor(0.3876),\n",
              "  tensor(0.3870),\n",
              "  tensor(0.3863),\n",
              "  tensor(0.3857),\n",
              "  tensor(0.3851),\n",
              "  tensor(0.3845),\n",
              "  tensor(0.3839),\n",
              "  tensor(0.3834),\n",
              "  tensor(0.3828),\n",
              "  tensor(0.3822),\n",
              "  tensor(0.3817),\n",
              "  tensor(0.3811),\n",
              "  tensor(0.3805),\n",
              "  tensor(0.3800),\n",
              "  tensor(0.3794),\n",
              "  tensor(0.3788),\n",
              "  tensor(0.3783),\n",
              "  tensor(0.3777),\n",
              "  tensor(0.3771),\n",
              "  tensor(0.3766),\n",
              "  tensor(0.3760),\n",
              "  tensor(0.3754),\n",
              "  tensor(0.3749),\n",
              "  tensor(0.3743),\n",
              "  tensor(0.3737),\n",
              "  tensor(0.3732),\n",
              "  tensor(0.3726),\n",
              "  tensor(0.3720),\n",
              "  tensor(0.3715),\n",
              "  tensor(0.3709),\n",
              "  tensor(0.3703),\n",
              "  tensor(0.3698),\n",
              "  tensor(0.3692),\n",
              "  tensor(0.3687),\n",
              "  tensor(0.3681),\n",
              "  tensor(0.3675),\n",
              "  tensor(0.3670),\n",
              "  tensor(0.3664),\n",
              "  tensor(0.3659),\n",
              "  tensor(0.3654),\n",
              "  tensor(0.3649),\n",
              "  tensor(0.3644),\n",
              "  tensor(0.3639),\n",
              "  tensor(0.3634),\n",
              "  tensor(0.3629),\n",
              "  tensor(0.3624),\n",
              "  tensor(0.3619),\n",
              "  tensor(0.3614),\n",
              "  tensor(0.3609),\n",
              "  tensor(0.3604),\n",
              "  tensor(0.3599),\n",
              "  tensor(0.3594),\n",
              "  tensor(0.3589),\n",
              "  tensor(0.3584),\n",
              "  tensor(0.3579),\n",
              "  tensor(0.3574),\n",
              "  tensor(0.3569),\n",
              "  tensor(0.3564),\n",
              "  tensor(0.3559),\n",
              "  tensor(0.3554),\n",
              "  tensor(0.3549),\n",
              "  tensor(0.3544),\n",
              "  tensor(0.3539),\n",
              "  tensor(0.3534),\n",
              "  tensor(0.3529),\n",
              "  tensor(0.3523),\n",
              "  tensor(0.3518),\n",
              "  tensor(0.3513),\n",
              "  tensor(0.3508),\n",
              "  tensor(0.3503),\n",
              "  tensor(0.3498),\n",
              "  tensor(0.3493),\n",
              "  tensor(0.3488),\n",
              "  tensor(0.3484),\n",
              "  tensor(0.3479),\n",
              "  tensor(0.3475),\n",
              "  tensor(0.3471),\n",
              "  tensor(0.3466),\n",
              "  tensor(0.3462),\n",
              "  tensor(0.3457),\n",
              "  tensor(0.3453),\n",
              "  tensor(0.3449),\n",
              "  tensor(0.3444),\n",
              "  tensor(0.3440),\n",
              "  tensor(0.3435),\n",
              "  tensor(0.3431),\n",
              "  tensor(0.3427),\n",
              "  tensor(0.3422),\n",
              "  tensor(0.3418),\n",
              "  tensor(0.3413),\n",
              "  tensor(0.3409),\n",
              "  tensor(0.3405),\n",
              "  tensor(0.3400),\n",
              "  tensor(0.3396),\n",
              "  tensor(0.3391),\n",
              "  tensor(0.3387),\n",
              "  tensor(0.3383),\n",
              "  tensor(0.3378),\n",
              "  tensor(0.3374),\n",
              "  tensor(0.3369),\n",
              "  tensor(0.3365),\n",
              "  tensor(0.3361),\n",
              "  tensor(0.3356),\n",
              "  tensor(0.3352),\n",
              "  tensor(0.3347),\n",
              "  tensor(0.3343),\n",
              "  tensor(0.3339),\n",
              "  tensor(0.3334),\n",
              "  tensor(0.3330),\n",
              "  tensor(0.3325),\n",
              "  tensor(0.3321),\n",
              "  tensor(0.3316),\n",
              "  tensor(0.3312),\n",
              "  tensor(0.3308),\n",
              "  tensor(0.3304),\n",
              "  tensor(0.3300),\n",
              "  tensor(0.3296),\n",
              "  tensor(0.3293),\n",
              "  tensor(0.3289),\n",
              "  tensor(0.3285),\n",
              "  tensor(0.3281),\n",
              "  tensor(0.3278),\n",
              "  tensor(0.3274),\n",
              "  tensor(0.3270),\n",
              "  tensor(0.3266),\n",
              "  tensor(0.3263),\n",
              "  tensor(0.3259),\n",
              "  tensor(0.3255),\n",
              "  tensor(0.3251),\n",
              "  tensor(0.3248),\n",
              "  tensor(0.3244),\n",
              "  tensor(0.3240),\n",
              "  tensor(0.3236),\n",
              "  tensor(0.3232),\n",
              "  tensor(0.3229),\n",
              "  tensor(0.3225),\n",
              "  tensor(0.3221),\n",
              "  tensor(0.3217),\n",
              "  tensor(0.3214),\n",
              "  tensor(0.3210),\n",
              "  tensor(0.3206),\n",
              "  tensor(0.3202),\n",
              "  tensor(0.3199),\n",
              "  tensor(0.3195),\n",
              "  tensor(0.3191),\n",
              "  tensor(0.3187),\n",
              "  tensor(0.3184),\n",
              "  tensor(0.3180),\n",
              "  tensor(0.3176),\n",
              "  tensor(0.3172),\n",
              "  tensor(0.3169),\n",
              "  tensor(0.3165),\n",
              "  tensor(0.3161),\n",
              "  tensor(0.3157),\n",
              "  tensor(0.3154),\n",
              "  tensor(0.3150),\n",
              "  tensor(0.3146),\n",
              "  tensor(0.3142),\n",
              "  tensor(0.3138),\n",
              "  tensor(0.3135),\n",
              "  tensor(0.3131),\n",
              "  tensor(0.3127),\n",
              "  tensor(0.3123),\n",
              "  tensor(0.3120),\n",
              "  tensor(0.3117),\n",
              "  tensor(0.3114),\n",
              "  tensor(0.3111),\n",
              "  tensor(0.3108),\n",
              "  tensor(0.3105),\n",
              "  tensor(0.3102),\n",
              "  tensor(0.3099),\n",
              "  tensor(0.3095),\n",
              "  tensor(0.3092),\n",
              "  tensor(0.3089),\n",
              "  tensor(0.3086),\n",
              "  tensor(0.3083),\n",
              "  tensor(0.3080),\n",
              "  tensor(0.3077),\n",
              "  tensor(0.3074),\n",
              "  tensor(0.3071),\n",
              "  tensor(0.3067),\n",
              "  tensor(0.3064),\n",
              "  tensor(0.3061),\n",
              "  tensor(0.3058),\n",
              "  tensor(0.3055),\n",
              "  tensor(0.3052),\n",
              "  tensor(0.3049),\n",
              "  tensor(0.3046),\n",
              "  tensor(0.3043),\n",
              "  tensor(0.3039),\n",
              "  tensor(0.3036),\n",
              "  tensor(0.3033),\n",
              "  tensor(0.3030),\n",
              "  tensor(0.3027),\n",
              "  tensor(0.3024),\n",
              "  tensor(0.3021),\n",
              "  tensor(0.3018),\n",
              "  tensor(0.3015),\n",
              "  tensor(0.3011),\n",
              "  tensor(0.3008),\n",
              "  tensor(0.3005),\n",
              "  tensor(0.3002),\n",
              "  tensor(0.2999),\n",
              "  tensor(0.2996),\n",
              "  tensor(0.2993),\n",
              "  tensor(0.2990),\n",
              "  tensor(0.2987),\n",
              "  tensor(0.2984),\n",
              "  tensor(0.2980),\n",
              "  tensor(0.2977),\n",
              "  tensor(0.2974),\n",
              "  tensor(0.2971),\n",
              "  tensor(0.2968),\n",
              "  tensor(0.2965),\n",
              "  tensor(0.2962),\n",
              "  tensor(0.2959),\n",
              "  tensor(0.2956),\n",
              "  tensor(0.2952),\n",
              "  tensor(0.2949),\n",
              "  tensor(0.2946),\n",
              "  tensor(0.2943),\n",
              "  tensor(0.2940),\n",
              "  tensor(0.2937),\n",
              "  tensor(0.2934),\n",
              "  tensor(0.2931),\n",
              "  tensor(0.2929),\n",
              "  tensor(0.2926),\n",
              "  tensor(0.2924),\n",
              "  tensor(0.2922),\n",
              "  tensor(0.2919),\n",
              "  tensor(0.2917),\n",
              "  tensor(0.2914),\n",
              "  tensor(0.2912),\n",
              "  tensor(0.2909),\n",
              "  tensor(0.2907),\n",
              "  tensor(0.2904),\n",
              "  tensor(0.2902),\n",
              "  tensor(0.2899),\n",
              "  tensor(0.2897),\n",
              "  tensor(0.2895),\n",
              "  tensor(0.2892),\n",
              "  tensor(0.2890),\n",
              "  tensor(0.2887),\n",
              "  tensor(0.2885),\n",
              "  tensor(0.2882),\n",
              "  tensor(0.2880),\n",
              "  tensor(0.2877),\n",
              "  tensor(0.2875),\n",
              "  tensor(0.2873),\n",
              "  tensor(0.2870),\n",
              "  tensor(0.2868),\n",
              "  tensor(0.2865),\n",
              "  tensor(0.2863),\n",
              "  tensor(0.2860),\n",
              "  tensor(0.2858),\n",
              "  tensor(0.2855),\n",
              "  tensor(0.2853),\n",
              "  tensor(0.2850),\n",
              "  tensor(0.2848),\n",
              "  tensor(0.2846),\n",
              "  tensor(0.2843),\n",
              "  tensor(0.2841),\n",
              "  tensor(0.2838),\n",
              "  tensor(0.2836),\n",
              "  tensor(0.2833),\n",
              "  tensor(0.2831),\n",
              "  tensor(0.2828),\n",
              "  tensor(0.2826),\n",
              "  tensor(0.2824),\n",
              "  tensor(0.2821),\n",
              "  tensor(0.2819),\n",
              "  tensor(0.2816),\n",
              "  tensor(0.2814),\n",
              "  tensor(0.2811),\n",
              "  tensor(0.2809),\n",
              "  tensor(0.2806),\n",
              "  tensor(0.2804),\n",
              "  tensor(0.2802),\n",
              "  tensor(0.2799),\n",
              "  tensor(0.2797),\n",
              "  tensor(0.2794),\n",
              "  tensor(0.2792),\n",
              "  tensor(0.2789),\n",
              "  tensor(0.2787),\n",
              "  tensor(0.2784),\n",
              "  tensor(0.2782),\n",
              "  tensor(0.2779),\n",
              "  tensor(0.2777),\n",
              "  tensor(0.2775),\n",
              "  tensor(0.2772),\n",
              "  tensor(0.2770),\n",
              "  tensor(0.2767),\n",
              "  tensor(0.2765),\n",
              "  tensor(0.2762),\n",
              "  tensor(0.2760),\n",
              "  tensor(0.2757),\n",
              "  tensor(0.2755),\n",
              "  tensor(0.2753),\n",
              "  tensor(0.2750),\n",
              "  tensor(0.2748),\n",
              "  tensor(0.2745),\n",
              "  tensor(0.2743),\n",
              "  tensor(0.2740),\n",
              "  tensor(0.2738),\n",
              "  tensor(0.2735),\n",
              "  tensor(0.2733),\n",
              "  tensor(0.2730),\n",
              "  tensor(0.2729),\n",
              "  tensor(0.2727),\n",
              "  tensor(0.2725),\n",
              "  tensor(0.2723),\n",
              "  tensor(0.2722),\n",
              "  tensor(0.2720),\n",
              "  tensor(0.2718),\n",
              "  tensor(0.2716),\n",
              "  tensor(0.2714),\n",
              "  tensor(0.2713),\n",
              "  tensor(0.2711),\n",
              "  tensor(0.2709),\n",
              "  tensor(0.2707),\n",
              "  tensor(0.2706),\n",
              "  tensor(0.2704),\n",
              "  tensor(0.2702),\n",
              "  tensor(0.2700),\n",
              "  tensor(0.2698),\n",
              "  tensor(0.2697),\n",
              "  tensor(0.2695),\n",
              "  tensor(0.2693),\n",
              "  tensor(0.2691),\n",
              "  tensor(0.2690),\n",
              "  tensor(0.2688),\n",
              "  tensor(0.2686),\n",
              "  tensor(0.2684),\n",
              "  tensor(0.2682),\n",
              "  tensor(0.2681),\n",
              "  tensor(0.2679),\n",
              "  tensor(0.2677),\n",
              "  tensor(0.2675),\n",
              "  tensor(0.2674),\n",
              "  tensor(0.2672),\n",
              "  tensor(0.2670),\n",
              "  tensor(0.2668),\n",
              "  tensor(0.2666),\n",
              "  tensor(0.2665),\n",
              "  tensor(0.2663),\n",
              "  tensor(0.2661),\n",
              "  tensor(0.2659),\n",
              "  tensor(0.2657),\n",
              "  tensor(0.2656),\n",
              "  tensor(0.2654),\n",
              "  tensor(0.2652),\n",
              "  tensor(0.2650),\n",
              "  tensor(0.2649),\n",
              "  tensor(0.2647),\n",
              "  tensor(0.2645),\n",
              "  tensor(0.2643),\n",
              "  tensor(0.2641),\n",
              "  tensor(0.2640),\n",
              "  tensor(0.2638),\n",
              "  tensor(0.2636),\n",
              "  tensor(0.2634),\n",
              "  tensor(0.2633),\n",
              "  tensor(0.2631),\n",
              "  tensor(0.2629),\n",
              "  tensor(0.2627),\n",
              "  tensor(0.2625),\n",
              "  tensor(0.2624),\n",
              "  tensor(0.2622),\n",
              "  tensor(0.2620),\n",
              "  tensor(0.2618),\n",
              "  tensor(0.2617),\n",
              "  tensor(0.2615),\n",
              "  tensor(0.2613),\n",
              "  tensor(0.2611),\n",
              "  tensor(0.2609),\n",
              "  tensor(0.2608),\n",
              "  tensor(0.2606),\n",
              "  tensor(0.2604),\n",
              "  tensor(0.2602),\n",
              "  tensor(0.2601),\n",
              "  tensor(0.2599),\n",
              "  tensor(0.2597),\n",
              "  tensor(0.2595),\n",
              "  tensor(0.2593),\n",
              "  tensor(0.2592),\n",
              "  tensor(0.2590),\n",
              "  tensor(0.2588),\n",
              "  tensor(0.2586),\n",
              "  tensor(0.2585),\n",
              "  tensor(0.2583),\n",
              "  tensor(0.2581),\n",
              "  tensor(0.2579),\n",
              "  tensor(0.2577),\n",
              "  tensor(0.2576),\n",
              "  tensor(0.2574),\n",
              "  tensor(0.2572),\n",
              "  tensor(0.2570),\n",
              "  tensor(0.2568),\n",
              "  tensor(0.2567),\n",
              "  tensor(0.2565),\n",
              "  tensor(0.2563),\n",
              "  tensor(0.2561),\n",
              "  tensor(0.2560),\n",
              "  tensor(0.2558),\n",
              "  tensor(0.2556),\n",
              "  tensor(0.2554),\n",
              "  tensor(0.2552),\n",
              "  tensor(0.2551),\n",
              "  tensor(0.2549),\n",
              "  tensor(0.2547),\n",
              "  tensor(0.2545),\n",
              "  tensor(0.2544),\n",
              "  tensor(0.2542),\n",
              "  tensor(0.2540),\n",
              "  tensor(0.2538),\n",
              "  tensor(0.2536),\n",
              "  tensor(0.2535),\n",
              "  tensor(0.2533),\n",
              "  tensor(0.2531),\n",
              "  tensor(0.2529),\n",
              "  tensor(0.2528),\n",
              "  tensor(0.2526),\n",
              "  tensor(0.2524),\n",
              "  tensor(0.2522),\n",
              "  tensor(0.2520),\n",
              "  tensor(0.2519),\n",
              "  tensor(0.2517),\n",
              "  tensor(0.2515),\n",
              "  tensor(0.2513),\n",
              "  tensor(0.2512),\n",
              "  tensor(0.2510),\n",
              "  tensor(0.2508),\n",
              "  tensor(0.2506),\n",
              "  tensor(0.2504),\n",
              "  tensor(0.2503),\n",
              "  tensor(0.2501),\n",
              "  tensor(0.2499),\n",
              "  tensor(0.2497),\n",
              "  tensor(0.2496),\n",
              "  tensor(0.2494),\n",
              "  tensor(0.2493),\n",
              "  tensor(0.2492),\n",
              "  tensor(0.2490),\n",
              "  tensor(0.2489),\n",
              "  tensor(0.2488),\n",
              "  tensor(0.2487),\n",
              "  tensor(0.2486),\n",
              "  tensor(0.2485),\n",
              "  tensor(0.2484),\n",
              "  tensor(0.2483),\n",
              "  tensor(0.2482),\n",
              "  tensor(0.2481),\n",
              "  tensor(0.2479),\n",
              "  tensor(0.2478),\n",
              "  tensor(0.2477),\n",
              "  tensor(0.2476),\n",
              "  tensor(0.2475),\n",
              "  tensor(0.2474),\n",
              "  tensor(0.2473),\n",
              "  tensor(0.2472),\n",
              "  tensor(0.2471),\n",
              "  tensor(0.2469),\n",
              "  tensor(0.2468),\n",
              "  tensor(0.2467),\n",
              "  tensor(0.2466),\n",
              "  tensor(0.2465),\n",
              "  tensor(0.2464),\n",
              "  tensor(0.2463),\n",
              "  tensor(0.2462),\n",
              "  tensor(0.2461),\n",
              "  tensor(0.2460),\n",
              "  tensor(0.2458),\n",
              "  tensor(0.2457),\n",
              "  tensor(0.2456),\n",
              "  tensor(0.2455),\n",
              "  tensor(0.2454),\n",
              "  tensor(0.2453),\n",
              "  tensor(0.2452),\n",
              "  tensor(0.2451),\n",
              "  tensor(0.2450),\n",
              "  tensor(0.2449),\n",
              "  tensor(0.2447),\n",
              "  tensor(0.2446),\n",
              "  tensor(0.2445),\n",
              "  tensor(0.2444),\n",
              "  tensor(0.2443),\n",
              "  tensor(0.2442),\n",
              "  tensor(0.2441),\n",
              "  tensor(0.2440),\n",
              "  tensor(0.2439),\n",
              "  tensor(0.2438),\n",
              "  tensor(0.2436),\n",
              "  tensor(0.2435),\n",
              "  tensor(0.2434),\n",
              "  tensor(0.2433),\n",
              "  tensor(0.2432),\n",
              "  tensor(0.2431),\n",
              "  tensor(0.2430),\n",
              "  tensor(0.2429),\n",
              "  tensor(0.2428),\n",
              "  tensor(0.2427),\n",
              "  tensor(0.2425),\n",
              "  tensor(0.2424),\n",
              "  tensor(0.2423),\n",
              "  tensor(0.2422),\n",
              "  tensor(0.2421),\n",
              "  tensor(0.2420),\n",
              "  tensor(0.2419),\n",
              "  tensor(0.2418),\n",
              "  tensor(0.2417),\n",
              "  tensor(0.2415),\n",
              "  tensor(0.2414),\n",
              "  tensor(0.2413),\n",
              "  tensor(0.2412),\n",
              "  tensor(0.2411),\n",
              "  tensor(0.2410),\n",
              "  tensor(0.2409),\n",
              "  tensor(0.2408),\n",
              "  tensor(0.2407),\n",
              "  tensor(0.2406),\n",
              "  tensor(0.2404),\n",
              "  tensor(0.2403),\n",
              "  tensor(0.2402),\n",
              "  tensor(0.2401),\n",
              "  tensor(0.2400),\n",
              "  tensor(0.2399),\n",
              "  tensor(0.2398),\n",
              "  tensor(0.2397),\n",
              "  tensor(0.2396),\n",
              "  tensor(0.2395),\n",
              "  tensor(0.2393),\n",
              "  tensor(0.2392),\n",
              "  tensor(0.2391),\n",
              "  tensor(0.2390),\n",
              "  tensor(0.2389),\n",
              "  tensor(0.2388),\n",
              "  tensor(0.2387),\n",
              "  tensor(0.2386),\n",
              "  tensor(0.2385),\n",
              "  tensor(0.2384),\n",
              "  tensor(0.2382),\n",
              "  tensor(0.2381),\n",
              "  tensor(0.2380),\n",
              "  tensor(0.2379),\n",
              "  tensor(0.2378),\n",
              "  tensor(0.2377),\n",
              "  tensor(0.2376),\n",
              "  tensor(0.2375),\n",
              "  tensor(0.2374),\n",
              "  tensor(0.2373),\n",
              "  tensor(0.2371),\n",
              "  tensor(0.2370),\n",
              "  tensor(0.2369),\n",
              "  tensor(0.2368),\n",
              "  tensor(0.2367),\n",
              "  tensor(0.2366),\n",
              "  tensor(0.2365),\n",
              "  tensor(0.2364),\n",
              "  tensor(0.2363),\n",
              "  tensor(0.2361),\n",
              "  tensor(0.2360),\n",
              "  tensor(0.2359),\n",
              "  tensor(0.2358),\n",
              "  tensor(0.2357),\n",
              "  tensor(0.2356),\n",
              "  tensor(0.2355),\n",
              "  tensor(0.2354),\n",
              "  tensor(0.2353),\n",
              "  tensor(0.2352),\n",
              "  tensor(0.2350),\n",
              "  tensor(0.2349),\n",
              "  tensor(0.2348),\n",
              "  tensor(0.2347),\n",
              "  tensor(0.2346),\n",
              "  tensor(0.2345),\n",
              "  tensor(0.2344),\n",
              "  tensor(0.2343),\n",
              "  tensor(0.2342),\n",
              "  tensor(0.2341),\n",
              "  tensor(0.2339),\n",
              "  tensor(0.2338),\n",
              "  tensor(0.2337),\n",
              "  tensor(0.2336),\n",
              "  tensor(0.2335),\n",
              "  tensor(0.2334),\n",
              "  tensor(0.2333),\n",
              "  tensor(0.2332),\n",
              "  tensor(0.2331),\n",
              "  tensor(0.2330),\n",
              "  tensor(0.2328),\n",
              "  tensor(0.2327),\n",
              "  tensor(0.2326),\n",
              "  tensor(0.2325),\n",
              "  tensor(0.2324),\n",
              "  tensor(0.2323),\n",
              "  tensor(0.2322),\n",
              "  tensor(0.2321),\n",
              "  tensor(0.2320),\n",
              "  tensor(0.2319),\n",
              "  tensor(0.2317),\n",
              "  tensor(0.2316),\n",
              "  tensor(0.2315),\n",
              "  tensor(0.2314),\n",
              "  tensor(0.2313),\n",
              "  tensor(0.2312),\n",
              "  tensor(0.2311),\n",
              "  tensor(0.2310),\n",
              "  tensor(0.2309),\n",
              "  tensor(0.2308),\n",
              "  tensor(0.2306),\n",
              "  tensor(0.2305),\n",
              "  tensor(0.2304),\n",
              "  tensor(0.2303),\n",
              "  tensor(0.2302),\n",
              "  tensor(0.2301),\n",
              "  tensor(0.2300),\n",
              "  tensor(0.2299),\n",
              "  tensor(0.2298),\n",
              "  tensor(0.2296),\n",
              "  tensor(0.2295),\n",
              "  tensor(0.2294),\n",
              "  tensor(0.2293),\n",
              "  tensor(0.2292),\n",
              "  tensor(0.2291),\n",
              "  tensor(0.2290),\n",
              "  tensor(0.2289),\n",
              "  tensor(0.2288),\n",
              "  tensor(0.2287),\n",
              "  tensor(0.2285),\n",
              "  tensor(0.2284),\n",
              "  tensor(0.2283),\n",
              "  tensor(0.2282),\n",
              "  tensor(0.2281),\n",
              "  tensor(0.2280),\n",
              "  tensor(0.2279),\n",
              "  tensor(0.2278),\n",
              "  tensor(0.2277),\n",
              "  tensor(0.2276),\n",
              "  tensor(0.2274),\n",
              "  tensor(0.2273),\n",
              "  tensor(0.2272),\n",
              "  tensor(0.2271),\n",
              "  tensor(0.2270),\n",
              "  tensor(0.2269),\n",
              "  tensor(0.2268),\n",
              "  tensor(0.2267),\n",
              "  tensor(0.2266),\n",
              "  tensor(0.2265),\n",
              "  tensor(0.2263),\n",
              "  tensor(0.2262),\n",
              "  tensor(0.2261),\n",
              "  tensor(0.2260),\n",
              "  tensor(0.2259),\n",
              "  tensor(0.2258),\n",
              "  tensor(0.2257),\n",
              "  tensor(0.2256),\n",
              "  tensor(0.2255),\n",
              "  tensor(0.2254),\n",
              "  tensor(0.2252),\n",
              "  tensor(0.2251),\n",
              "  tensor(0.2250),\n",
              "  tensor(0.2249),\n",
              "  tensor(0.2248),\n",
              "  tensor(0.2247),\n",
              "  tensor(0.2246),\n",
              "  tensor(0.2245),\n",
              "  tensor(0.2244),\n",
              "  tensor(0.2242),\n",
              "  tensor(0.2241),\n",
              "  tensor(0.2240),\n",
              "  tensor(0.2239),\n",
              "  tensor(0.2238),\n",
              "  tensor(0.2237),\n",
              "  tensor(0.2236),\n",
              "  tensor(0.2235),\n",
              "  tensor(0.2234),\n",
              "  tensor(0.2233),\n",
              "  tensor(0.2231),\n",
              "  tensor(0.2230),\n",
              "  tensor(0.2229),\n",
              "  tensor(0.2228),\n",
              "  tensor(0.2227),\n",
              "  tensor(0.2226),\n",
              "  tensor(0.2225),\n",
              "  tensor(0.2224),\n",
              "  tensor(0.2223),\n",
              "  tensor(0.2222),\n",
              "  tensor(0.2220),\n",
              "  tensor(0.2219),\n",
              "  tensor(0.2218),\n",
              "  tensor(0.2217),\n",
              "  tensor(0.2216),\n",
              "  tensor(0.2215),\n",
              "  tensor(0.2214),\n",
              "  tensor(0.2213),\n",
              "  tensor(0.2212),\n",
              "  tensor(0.2211),\n",
              "  tensor(0.2209),\n",
              "  tensor(0.2208),\n",
              "  tensor(0.2207),\n",
              "  tensor(0.2206),\n",
              "  tensor(0.2205),\n",
              "  tensor(0.2204),\n",
              "  tensor(0.2203),\n",
              "  tensor(0.2202),\n",
              "  tensor(0.2201),\n",
              "  tensor(0.2200),\n",
              "  tensor(0.2198),\n",
              "  tensor(0.2197),\n",
              "  tensor(0.2196),\n",
              "  tensor(0.2195),\n",
              "  tensor(0.2194),\n",
              "  tensor(0.2193),\n",
              "  tensor(0.2192),\n",
              "  tensor(0.2191),\n",
              "  tensor(0.2190),\n",
              "  tensor(0.2188),\n",
              "  tensor(0.2187),\n",
              "  tensor(0.2186),\n",
              "  tensor(0.2185),\n",
              "  tensor(0.2184),\n",
              "  tensor(0.2183),\n",
              "  tensor(0.2182),\n",
              "  tensor(0.2181),\n",
              "  tensor(0.2180),\n",
              "  tensor(0.2179),\n",
              "  tensor(0.2177),\n",
              "  tensor(0.2176),\n",
              "  tensor(0.2175),\n",
              "  tensor(0.2174),\n",
              "  tensor(0.2173),\n",
              "  tensor(0.2172),\n",
              "  tensor(0.2171),\n",
              "  tensor(0.2170),\n",
              "  tensor(0.2169),\n",
              "  tensor(0.2168),\n",
              "  tensor(0.2166),\n",
              "  tensor(0.2165),\n",
              "  tensor(0.2164),\n",
              "  tensor(0.2163),\n",
              "  tensor(0.2162),\n",
              "  tensor(0.2161),\n",
              "  tensor(0.2160),\n",
              "  tensor(0.2159),\n",
              "  tensor(0.2158),\n",
              "  tensor(0.2157),\n",
              "  tensor(0.2155),\n",
              "  tensor(0.2154),\n",
              "  tensor(0.2153),\n",
              "  tensor(0.2152),\n",
              "  tensor(0.2151),\n",
              "  tensor(0.2150),\n",
              "  tensor(0.2149),\n",
              "  tensor(0.2148),\n",
              "  tensor(0.2147),\n",
              "  tensor(0.2146),\n",
              "  tensor(0.2144),\n",
              "  tensor(0.2143),\n",
              "  tensor(0.2142),\n",
              "  tensor(0.2141),\n",
              "  tensor(0.2140),\n",
              "  tensor(0.2139),\n",
              "  tensor(0.2138),\n",
              "  tensor(0.2137),\n",
              "  tensor(0.2136),\n",
              "  tensor(0.2134),\n",
              "  tensor(0.2133),\n",
              "  tensor(0.2132),\n",
              "  tensor(0.2131),\n",
              "  tensor(0.2130),\n",
              "  tensor(0.2129),\n",
              "  tensor(0.2128),\n",
              "  tensor(0.2127),\n",
              "  tensor(0.2126),\n",
              "  tensor(0.2125),\n",
              "  tensor(0.2123),\n",
              "  tensor(0.2122),\n",
              "  tensor(0.2121),\n",
              "  tensor(0.2120),\n",
              "  tensor(0.2119),\n",
              "  tensor(0.2118),\n",
              "  tensor(0.2117),\n",
              "  tensor(0.2116),\n",
              "  tensor(0.2115),\n",
              "  tensor(0.2114),\n",
              "  tensor(0.2112),\n",
              "  tensor(0.2111),\n",
              "  tensor(0.2110),\n",
              "  tensor(0.2109),\n",
              "  tensor(0.2108),\n",
              "  tensor(0.2107),\n",
              "  tensor(0.2106),\n",
              "  tensor(0.2105),\n",
              "  tensor(0.2104),\n",
              "  tensor(0.2103),\n",
              "  tensor(0.2101),\n",
              "  tensor(0.2100),\n",
              "  tensor(0.2099),\n",
              "  tensor(0.2098),\n",
              "  tensor(0.2097),\n",
              "  tensor(0.2096),\n",
              "  tensor(0.2095),\n",
              "  tensor(0.2094),\n",
              "  tensor(0.2093),\n",
              "  tensor(0.2092),\n",
              "  tensor(0.2090),\n",
              "  tensor(0.2089),\n",
              "  tensor(0.2088),\n",
              "  tensor(0.2087),\n",
              "  tensor(0.2086),\n",
              "  tensor(0.2085),\n",
              "  tensor(0.2084),\n",
              "  tensor(0.2083),\n",
              "  tensor(0.2082),\n",
              "  tensor(0.2080),\n",
              "  tensor(0.2079),\n",
              "  tensor(0.2078),\n",
              "  tensor(0.2077),\n",
              "  tensor(0.2076),\n",
              "  tensor(0.2075),\n",
              "  tensor(0.2074),\n",
              "  tensor(0.2073),\n",
              "  tensor(0.2072),\n",
              "  tensor(0.2071),\n",
              "  tensor(0.2069),\n",
              "  tensor(0.2068),\n",
              "  tensor(0.2067),\n",
              "  tensor(0.2066),\n",
              "  tensor(0.2065),\n",
              "  tensor(0.2064),\n",
              "  tensor(0.2063),\n",
              "  tensor(0.2062),\n",
              "  tensor(0.2061),\n",
              "  tensor(0.2060),\n",
              "  tensor(0.2058),\n",
              "  tensor(0.2057),\n",
              "  tensor(0.2056),\n",
              "  tensor(0.2055),\n",
              "  tensor(0.2055),\n",
              "  tensor(0.2054),\n",
              "  tensor(0.2053),\n",
              "  tensor(0.2052),\n",
              "  tensor(0.2052),\n",
              "  tensor(0.2051),\n",
              "  tensor(0.2050),\n",
              "  tensor(0.2049),\n",
              "  tensor(0.2048),\n",
              "  tensor(0.2048),\n",
              "  tensor(0.2046),\n",
              "  tensor(0.2046),\n",
              "  tensor(0.2045),\n",
              "  tensor(0.2045),\n",
              "  tensor(0.2043),\n",
              "  tensor(0.2042),\n",
              "  tensor(0.2042),\n",
              "  tensor(0.2041),\n",
              "  tensor(0.2040),\n",
              "  tensor(0.2039),\n",
              "  tensor(0.2039),\n",
              "  tensor(0.2038),\n",
              "  tensor(0.2037),\n",
              "  tensor(0.2036),\n",
              "  tensor(0.2035),\n",
              "  tensor(0.2035),\n",
              "  tensor(0.2034),\n",
              "  tensor(0.2033),\n",
              "  tensor(0.2032),\n",
              "  tensor(0.2032),\n",
              "  tensor(0.2031),\n",
              "  tensor(0.2030),\n",
              "  tensor(0.2029),\n",
              "  tensor(0.2028),\n",
              "  tensor(0.2028),\n",
              "  tensor(0.2026),\n",
              "  tensor(0.2026),\n",
              "  tensor(0.2025),\n",
              "  tensor(0.2024),\n",
              "  tensor(0.2023),\n",
              "  ...])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "plt.plot(epoch_count, np.array(torch.tensor(loss_values).numpy()), label=\"Train loss\")\n",
        "plt.plot(epoch_count, np.array(torch.tensor(test_loss_values).numpy()), label=\"Test loss\")\n",
        "plt.title(\"Training and test loss curves\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "I0Vn1oQvxAzJ",
        "outputId": "4595a92a-daee-4e7c-ef56-138449da6928"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x784dae38f070>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving a model in pytorch\n",
        "\n",
        "# 3 main methods for saving and loading models in pytorch\n",
        "# 1 - torch.save() - save pytorch object in pythons pickle format\n",
        "# 2 - torch.load() - load a saved pytorch object\n",
        "# 3 - torch.nn.Module.load_state_dict() - load the models save state dictionary"
      ],
      "metadata": {
        "id": "rPBbY3jkxL9G"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model:\n",
        "import pathlib as path\n",
        "import torch\n",
        "\n",
        "# Create model directory\n",
        "MODEL_PATH = path.Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Create model save path\n",
        "MODEL_NAME = \"01_pytorch_workflow_model_0.pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "\n",
        "# Save the model state dict\n",
        "torch.save(obj=model_0.state_dict(), f=MODEL_SAVE_PATH)"
      ],
      "metadata": {
        "id": "pSOaDo401iN7"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRne1wf62U0_",
        "outputId": "7b3bfe41-00f7-4ef9-83e9-d7796fd25fff"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "01_pytorch_workflow_model_0.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading a pytorch model\n",
        "# Since we saved our models state dict we need to instantiate a new class of our model\n",
        "MODEL_SAVE_PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2bsvLdq27n3",
        "outputId": "8883b9a6-4caf-49cb-9f38-6dd1e773820e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('models/01_pytorch_workflow_model_0.pth')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_module_0 = LinearRegressionModel()\n",
        "\n",
        "# Load the saved state_dict of model_0\n",
        "loaded_module_0.load_state_dict(torch.load(MODEL_SAVE_PATH))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ti4Iq9rD3l6P",
        "outputId": "e8aaa5a2-d05b-4bf4-c259-96518ff893e9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_module_0.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Y8ifIMm5vTe",
        "outputId": "67c50f98-793e-4ba1-c1f1-54bc795c52a7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.6999])), ('bias', tensor([0.3010]))])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make some predictions\n",
        "loaded_module_0.eval()\n",
        "with torch.inference_mode():\n",
        "  loaded_model_preds = loaded_module_0(X_test)\n",
        "\n",
        "loaded_model_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLS6ByjN6Goc",
        "outputId": "ea2ed87f-28f2-463d-8f7e-637b0646fc72"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8608],\n",
              "        [0.8748],\n",
              "        [0.8888],\n",
              "        [0.9028],\n",
              "        [0.9168],\n",
              "        [0.9308],\n",
              "        [0.9448],\n",
              "        [0.9588],\n",
              "        [0.9728],\n",
              "        [0.9868]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compare loaded model vs original model\n",
        "y_preds == loaded_model_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KklCPl336Uus",
        "outputId": "875f162f-5019-43a9-a3ee-fb3e1ce486e4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Putting it all together\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "Zuy-X1jl6ZVC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "o3qqhRSlZEA4",
        "outputId": "4f37d036-d8da-442e-992c-883577604547"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLC75xZ4ZXsz",
        "outputId": "d12f9cae-df1d-4839-94ff-a174342f94ce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Aug  2 09:23:21 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P8              10W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MqPDQuTxZp9F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}